<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>CVPR 2026 Workshop on Any-to-Any Multimodal Learning</title>
  <style>
    :root{
      --primary:#9b0d13;
      --bg:#0c0c0e;
      --card:#141418;
      --text:#f2f2f2;
      --muted:#b9b9c0;
      --line:#26262a;
      --radius:14px;
      --max:1100px;
    }
    *{box-sizing:border-box}
    body{
      margin:0;
      font:16px/1.65 system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;
      background:var(--bg);
      color:var(--text);
    }
    .container{
      max-width:var(--max);
      margin:0 auto;
      padding:28px 18px;
    }
    header{
      border-bottom:1px solid var(--line);
      background:linear-gradient(180deg,rgba(155,13,19,.25),transparent 70%);
    }
    h1,h2,h3{
      color:#fff;
      line-height:1.25;
    }
    h1{font-size:34px;margin:0 0 10px}
    h2{font-size:24px;margin:34px 0 12px}
    h3{font-size:18px;margin:22px 0 8px}
    p{margin:10px 0;color:var(--text)}
    ul{margin:10px 0 10px 22px}
    li{margin:6px 0}
    .card{
      background:var(--card);
      border:1px solid var(--line);
      border-radius:var(--radius);
      padding:22px;
      margin:18px 0;
    }
    .tag{
      display:inline-block;
      padding:6px 12px;
      border-radius:999px;
      background:rgba(155,13,19,.2);
      border:1px solid rgba(155,13,19,.6);
      color:#ffd7da;
      font-weight:600;
      margin-bottom:12px;
    }
    .section{
      margin-top:40px;
    }
    footer{
      border-top:1px solid var(--line);
      margin-top:50px;
      padding:24px 18px;
      color:var(--muted);
      font-size:13px;
    }
  </style>
</head>

<body>
<header>
  <div class="container">
    <span class="tag">CVPR 2026 Workshop Proposal</span>
    <h1>WORKSHOP ON ANY-TO-ANY MULTIMODAL LEARNING</h1>
    <p>
      Shengqiong Wu, Wei Dai, Han Lin, Chenyu (Monica) Wang, Yichen Li, Sharut Gupta,
      Roman Bachmann, Elisa Ricci
    </p>
    <p>
      National University of Singapore, MIT, University of North Carolina at Chapel Hill,
      Swiss Federal Institute of Technology (EPFL), University of Trento
    </p>
  </div>
</header>

<main class="container">

<div class="section card">
<h2>ABSTRACT</h2>
<p>
The recent surge of multimodal large models has brought unprecedented progress in connecting
language, vision, audio, and beyond. Yet, despite their impressive performance, most existing
systems remain constrained to fixed modality pairs, lacking the flexibility to generalize or reason
across arbitrary modality combinations. This limitation stands in contrast to human cognition,
which fluidly integrates diverse sensory channels to build a coherent understanding of the world.
The Any-to-Any Multimodal Learning (A2A-MML) workshop aims to explore the next
frontier of multimodal intelligence—developing systems capable of understanding, aligning,
transforming, and generating across any set of modalities. We organize the discussion around
three foundational pillars: (1) Multimodal representation learning, which seeks to learn
generalizable and disentangled representations that capture both shared and modality-specific
information; (2) Multimodal transformation, which investigates architectures and principles
enabling seamless translation, fusion, and adaptation across heterogeneous modalities; and (3)
Multimodal collaboration, which studies how modalities cooperate, complement, or conflict
with each other to achieve coherent reasoning and world modeling. By bringing together
researchers from vision, language, audio, 3D, robotics, and cognitive science, this workshop
aims to establish a roadmap toward any-to-any multimodal intelligence, where machines can
perceive, reason, and communicate across modalities as fluidly as humans do.
</p>
</div>

<div class="section card">
<h2>1 WORKSHOP SUMMARY</h2>

<h3>1.1 BACKGROUND AND MOTIVATION</h3>
<p>
Multimodal learning has rapidly evolved from early dual-modality systems (e.g., text–image) to
powerful multimodal foundation models that integrate vision, language, and audio. However, the
community is now facing a crucial inflection point. Despite significant progress, today’s models
still operate under narrow and rigid modality pairings, limited supervision settings, and task-specific
adaptation schemes. This rigidity prevents them from achieving the combinatorial generalization that
underlies human-like perception and reasoning. Humans, by contrast, seamlessly integrate multiple
sensory modalities—sight, sound, touch, spatial awareness, and language—into coherent,
context-aware understanding. Realizing this capability in AI requires a paradigm shift from
“multi-to-one” or “one-to-one” fusion toward any-to-any multimodal learning.
</p>
<p>
The A2A-MML workshop is motivated by three core needs currently underserved in the community:
1) Mechanistic and representational understanding, 2) Broader modality coverage and flexible
interaction, and 3) Methodological openness beyond foundation models. In uniting these perspectives,
A2A-MML aims to serve as a cross-disciplinary bridge, linking representation learning, cognitive
modeling, embodied AI, and generative modeling.
</p>
<p>
Tagline. Bridging Representation, Transformation, and Collaboration Toward Any-to-Any Multimodal Intelligence<br/>
Workshop Acronym. A2A-MML: Any-to-Any Multimodal Learning
</p>
</div>

<div class="section card">
<h3>1.2 TOPICS AND THEMES</h3>
<p>
We welcome all relevant submissions in the area of multimodal learning, with particular emphasis on
emerging advances and open challenges in any-to-any multimodal intelligence, such as:
</p>
<ul>
<li>Multimodal Representation Learning</li>
<li>Multimodal Transformation</li>
<li>Multimodal Synergtic Collaboration</li>
<li>Benchmarking and Evaluation for Any-to-Any Multimodal learning</li>
</ul>
</div>

<div class="section card">
<h3>1.3 BROADER IMPACT STATEMENT</h3>
<p>
This workshop aims to cultivate a deeper and more unified understanding of multimodal intelligence,
beyond the current siloed vision–language paradigm. By emphasizing the mechanisms behind
alignment, transformation, and collaboration, we hope to inspire research that is not only more
generalizable and interpretable but also more grounded in real-world multimodal diversity.
</p>
</div>

<div class="section card">
<h2>2 ORGANIZER COMMITTEE</h2>

<h3>2.1 ORGANIZER BIO</h3>
<p>
Shengqiong Wu (she/her) is a senior Ph.D. student at the National University of Singapore, under
the supervision of Prof. Tat-Seng Chua. Her research interests include multimodal learning and
large vision-language foundation models.
</p>

<p>
Wei Dai (he/him) is a Ph.D. student at MIT Media Lab under the supervision of Prof. Paul Liang.
His research interests lie in the field of multimodal learning, particularly multimodal LLMs,
and their applications to healthcare.
</p>

<p>
Han Lin (he/him) is a third-year Ph.D. student at the University of North Carolina at Chapel Hill,
supervised by Prof. Mohit Bansal.
</p>

<p>
Yichen Li (he/him) is a Ph.D. student at MIT, working with Prof. Antonio Torralba.
</p>

<p>
Chenyu (Monica) Wang (she/her) is a Ph.D. student at MIT, advised by Prof. Tommi Jaakkola.
</p>

<p>
Sharut Gupta (she/her) is a Ph.D. student at MIT, working with Prof. Phillip Isola and Prof.
Stefanie Jegelka.
</p>

<p>
Roman Bachmann (he/him) is a Ph.D. student at EPFL, advised by Prof. Amir Zamir.
</p>

<p>
Elisa Ricci (she/her) is a Professor at the University of Trento.
</p>
</div>

<div class="section card">
<h2>3 WORKSHOP FORMAT AND CONTENT</h2>

<h3>3.1 TENTATIVE SCHEDULE</h3>
<p>
We plan to host a hybrid workshop format, accommodating both onsite and online participation.
</p>
<p>
Important Dates for Review Process.
</p>
<ul>
<li>Workshop paper submission deadline: March 20, 2026.</li>
<li>Workshop paper notification date: April 5, 2026</li>
<li>Final workshop program, camera-ready, videos uploaded: April 12, 2026</li>
</ul>
</div>

<div class="section card">
<h3>3.3 PAPER SUBMISSION AND ACCEPTANCE</h3>
<p>
All submissions must be written in English, follow the official CVPR proceedings format, and adhere
to the double-blind review policy.
</p>
</div>

</main>

<footer>
  <div class="container">
    CVPR 2026 Workshop on Any-to-Any Multimodal Learning
  </div>
</footer>
</body>
</html>
