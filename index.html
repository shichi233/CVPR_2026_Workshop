<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>CVPR 2026 Workshop on Any-to-Any Multimodal Learning</title>
  <meta name="description" content="WORKSHOP ON ANY-TO-ANY MULTIMODAL LEARNING (A2A-MML) — CVPR 2026 Workshop Proposal" />
  <style>
    :root{
      --gunn-red:#9b0d13;
      --gunn-red-dark:#6f0a0f;
      --paper:#f6f3ef;
      --ink:#0f0f12;
      --muted:#5e5a55;
      --line:#e9e4dc;
      --card:#ffffff;
      --radius:18px;
      --shadow:0 8px 24px rgba(0,0,0,.10);
      --max:1100px;
    }

    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0;
      font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,"Helvetica Neue",Arial,"Noto Sans","Apple Color Emoji","Segoe UI Emoji";
      background:var(--paper);
      color:var(--ink);
      line-height:1.55;
    }
    a{color:inherit;text-decoration:none}
    .container{max-width:var(--max);margin:0 auto;padding:0 20px}

    .nav{
      position:sticky;
      top:0;
      z-index:50;
      backdrop-filter:saturate(1.2) blur(6px);
      background:rgba(246,243,239,.78);
      border-bottom:1px solid var(--line);
    }
    .nav-inner{
      display:flex;
      align-items:center;
      justify-content:space-between;
      gap:14px;
      padding:10px 0;
    }
    .brand{
      display:flex;
      align-items:center;
      gap:12px;
      font-weight:900;
      letter-spacing:.2px;
      white-space:nowrap;
    }
    .mark{
      width:40px;
      height:40px;
      border-radius:12px;
      background:linear-gradient(135deg,var(--gunn-red),var(--gunn-red-dark));
      box-shadow:0 2px 10px rgba(0,0,0,.12);
    }
    .menu{
      display:flex;
      align-items:center;
      gap:16px;
      flex-wrap:wrap;
      font-weight:650;
    }
    .menu a{
      opacity:.85;
      text-underline-offset:6px;
    }
    .menu a:hover{opacity:1;text-decoration:underline}

    .hero{
      background:var(--gunn-red);
      color:#fff;
    }
    .hero .wrap{
      display:grid;
      grid-template-columns:1.2fr 1fr;
      gap:28px;
      align-items:center;
      padding:52px 20px 62px;
    }
    .badge{
      display:inline-flex;
      align-items:center;
      gap:10px;
      padding:8px 14px;
      border-radius:999px;
      background:rgba(255,255,255,.12);
      border:1px solid rgba(255,255,255,.25);
      font-weight:800;
    }
    .hero h1{
      margin:14px 0 10px;
      font-size:clamp(28px,5.6vw,52px);
      line-height:1.06;
      letter-spacing:.2px;
    }
    .hero p{
      margin:8px 0 0;
      opacity:.95;
      font-size:clamp(15px,1.6vw,18px);
    }
    .hero-card{
      background:linear-gradient(160deg,rgba(255,255,255,.08),rgba(255,255,255,.02));
      border:1px solid rgba(255,255,255,.22);
      border-radius:var(--radius);
      padding:20px;
      box-shadow:var(--shadow);
    }
    .hero-card h2{
      margin:0 0 10px;
      font-size:16px;
      letter-spacing:.2px;
    }
    .hero-card .small{
      opacity:.95;
      font-weight:700;
      font-size:13px;
      line-height:1.6;
      margin:0;
    }

    section{padding:58px 0}
    .section-title{
      margin:0 0 12px;
      font-size:clamp(22px,3.2vw,34px);
      color:var(--ink);
      letter-spacing:.2px;
    }
    .sub{
      margin:0 0 18px;
      color:var(--muted);
      font-weight:600;
    }
    .card{
      background:var(--card);
      border:1px solid var(--line);
      border-radius:var(--radius);
      padding:22px;
      box-shadow:var(--shadow);
    }

    .pdftext{
      margin:0;
      white-space:pre-wrap;
      word-break:break-word;
      font:15px/1.6 ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;
      color:var(--ink);
    }

    footer{
      background:#111;
      color:#fff;
      margin-top:40px;
    }
    footer .container{
      padding:26px 20px;
      display:flex;
      justify-content:space-between;
      align-items:center;
      gap:16px;
      flex-wrap:wrap;
    }
    footer .muted{opacity:.85;font-weight:650}

    @media (max-width: 900px){
      .hero .wrap{grid-template-columns:1fr}
      .menu{gap:12px}
    }
  </style>
</head>

<body>
  <nav class="nav" aria-label="Primary">
    <div class="container nav-inner">
      <a class="brand" href="#top">
        <span class="mark" aria-hidden="true"></span>
        <span>A2A-MML · CVPR 2026</span>
      </a>
      <div class="menu">
        <a href="#proposal">Proposal</a>
        <a href="#organizers">Organizers</a>
        <a href="#format">Format</a>
        <a href="#references">References</a>
      </div>
    </div>
  </nav>

  <header id="top" class="hero">
    <div class="container wrap">
      <div>
        <span class="badge">WORKSHOP ON ANY-TO-ANY MULTIMODAL LEARNING</span>
        <h1>CVPR 2026 Workshop Proposal</h1>
        <p>Styling follows the same border and paper aesthetic you linked, with Gunn red as the accent.</p>
      </div>
      <div class="hero-card">
        <h2>Primary color</h2>
        <p class="small">#9b0d13</p>
        <h2 style="margin-top:14px">Theme</h2>
        <p class="small">paper background · black text · light borders · soft shadows · rounded cards</p>
      </div>
    </div>
  </header>

  <section id="proposal">
    <div class="container">
      <h2 class="section-title">Abstract</h2>
      <p class="sub">Content below is placed as-is from your PDF.</p>
      <div class="card">
        <pre class="pdftext">The recent surge of multimodal large models has brought unprecedented progress in connecting
language, vision, audio, and beyond. Yet, despite their impressive performance, most existing
systems remain constrained to fixed modality pairs, lacking the flexibility to generalize or reason
across arbitrary modality combinations. This limitation stands in contrast to human cognition,
which fluidly integrates diverse sensory channels to build a coherent understanding of the world.
The Any-to-Any Multimodal Learning (A2A-MML) workshop aims to explore the next
frontier of multimodal intelligence—developing systems capable of understanding, aligning,
transforming, and generating across any set of modalities. We organize the discussion around
three foundational pillars: (1) Multimodal representation learning, which seeks to learn
generalizable and disentangled representations that capture both shared and modality-specific
information; (2) Multimodal transformation, which investigates architectures and principles
enabling seamless translation, fusion, and adaptation across heterogeneous modalities; and (3)
Multimodal collaboration, which studies how modalities cooperate, complement, or conflict
with each other to achieve coherent reasoning and world modeling. By bringing together
researchers from vision, language, audio, 3D, robotics, and cognitive science, this workshop
aims to establish a roadmap toward any-to-any multimodal intelligence, where machines can
perceive, reason, and communicate across modalities as fluidly as humans do.</pre>
      </div>
    </div>
  </section>

  <section id="organizers">
    <div class="container">
      <h2 class="section-title">Organizers</h2>
      <p class="sub">Text is included in the proposal section above.</p>
    </div>
  </section>

  <section id="format">
    <div class="container">
      <h2 class="section-title">Workshop Format</h2>
      <p class="sub">Text is included in the proposal section above.</p>
    </div>
  </section>

  <section id="references">
    <div class="container">
      <h2 class="section-title">References</h2>
      <p class="sub">Text is included in the proposal section above.</p>
    </div>
  </section>

  <footer>
    <div class="container">
      <div class="muted">CVPR 2026 Workshop on Any-to-Any Multimodal Learning</div>
      <div class="muted">Theme: paper · ink · gunn red</div>
    </div>
  </footer>
</body>
</html>
