<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>CVPR 2026 Workshop on Any-to-Any Multimodal Learning</title>
  <meta name="description" content="WORKSHOP ON ANY-TO-ANY MULTIMODAL LEARNING (A2A-MML) — CVPR 2026 Workshop Proposal" />
  <style>
    :root{
      --gunn-red:#9b0d13;
      --gunn-red-dark:#6f0a0f;
      --paper:#f6f3ef;
      --ink:#0f0f12;
      --muted:#5e5a55;
      --line:#e9e4dc;
      --card:#ffffff;
      --radius:18px;
      --shadow:0 8px 24px rgba(0,0,0,.10);
      --max:1100px;
    }

    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0;
      font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,"Helvetica Neue",Arial,"Noto Sans","Apple Color Emoji","Segoe UI Emoji";
      background:var(--paper);
      color:var(--ink);
      line-height:1.55;
    }
    a{color:inherit;text-decoration:none}
    .container{max-width:var(--max);margin:0 auto;padding:0 20px}

    .nav{
      position:sticky;
      top:0;
      z-index:50;
      backdrop-filter:saturate(1.2) blur(6px);
      background:rgba(246,243,239,.78);
      border-bottom:1px solid var(--line);
    }
    .nav-inner{
      display:flex;
      align-items:center;
      justify-content:space-between;
      gap:14px;
      padding:10px 0;
    }
    .brand{
      display:flex;
      align-items:center;
      gap:12px;
      font-weight:900;
      letter-spacing:.2px;
      white-space:nowrap;
    }
    .mark{
      width:40px;
      height:40px;
      border-radius:12px;
      background:linear-gradient(135deg,var(--gunn-red),var(--gunn-red-dark));
      box-shadow:0 2px 10px rgba(0,0,0,.12);
    }
    .menu{
      display:flex;
      align-items:center;
      gap:16px;
      flex-wrap:wrap;
      font-weight:650;
    }
    .menu a{
      opacity:.85;
      text-underline-offset:6px;
    }
    .menu a:hover{opacity:1;text-decoration:underline}

    .hero{
      background:var(--gunn-red);
      color:#fff;
    }
    .hero .wrap{
      display:grid;
      grid-template-columns:1.2fr 1fr;
      gap:28px;
      align-items:center;
      padding:52px 20px 62px;
    }
    .badge{
      display:inline-flex;
      align-items:center;
      gap:10px;
      padding:8px 14px;
      border-radius:999px;
      background:rgba(255,255,255,.12);
      border:1px solid rgba(255,255,255,.25);
      font-weight:800;
    }
    .hero h1{
      margin:14px 0 10px;
      font-size:clamp(28px,5.6vw,52px);
      line-height:1.06;
      letter-spacing:.2px;
    }
    .hero p{
      margin:8px 0 0;
      opacity:.95;
      font-size:clamp(15px,1.6vw,18px);
    }
    .hero-card{
      background:linear-gradient(160deg,rgba(255,255,255,.08),rgba(255,255,255,.02));
      border:1px solid rgba(255,255,255,.22);
      border-radius:var(--radius);
      padding:20px;
      box-shadow:var(--shadow);
    }
    .hero-card h2{
      margin:0 0 10px;
      font-size:16px;
      letter-spacing:.2px;
    }
    .hero-card .small{
      opacity:.95;
      font-weight:700;
      font-size:13px;
      line-height:1.6;
      margin:0;
    }

    section{padding:58px 0}
    .section-title{
      margin:0 0 12px;
      font-size:clamp(22px,3.2vw,34px);
      color:var(--ink);
      letter-spacing:.2px;
    }
    .sub{
      margin:0 0 18px;
      color:var(--muted);
      font-weight:600;
    }
    .card{
      background:var(--card);
      border:1px solid var(--line);
      border-radius:var(--radius);
      padding:22px;
      box-shadow:var(--shadow);
    }

    .pdftext{
      margin:0;
      white-space:pre-wrap;
      word-break:break-word;
      font:15px/1.6 ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;
      color:var(--ink);
    }

    footer{
      background:#111;
      color:#fff;
      margin-top:40px;
    }
    footer .container{
      padding:26px 20px;
      display:flex;
      justify-content:space-between;
      align-items:center;
      gap:16px;
      flex-wrap:wrap;
    }
    footer .muted{opacity:.85;font-weight:650}

    @media (max-width: 900px){
      .hero .wrap{grid-template-columns:1fr}
      .menu{gap:12px}
    }
  </style>
</head>

<body>
  <nav class="nav" aria-label="Primary">
    <div class="container nav-inner">
      <a class="brand" href="#top">
        <span class="mark" aria-hidden="true"></span>
        <span>A2A-MML · CVPR 2026</span>
      </a>
      <div class="menu">
        <a href="#proposal">Proposal</a>
        <a href="#organizers">Organizers</a>
        <a href="#format">Format</a>
        <a href="#references">References</a>
      </div>
    </div>
  </nav>

  <header id="top" class="hero">
    <div class="container wrap">
      <div>
        <span class="badge">WORKSHOP ON ANY-TO-ANY MULTIMODAL LEARNING</span>
        <h1>CVPR 2026 Workshop Proposal</h1>
        <p>Styling follows the same border and paper aesthetic you linked, with Gunn red as the accent.</p>
      </div>
      <div class="hero-card">
        <h2>Primary color</h2>
        <p class="small">#9b0d13</p>
        <h2 style="margin-top:14px">Theme</h2>
        <p class="small">paper background · black text · light borders · soft shadows · rounded cards</p>
      </div>
    </div>
  </header>

  <section id="proposal">
    <div class="container">
      <h2 class="section-title">Workshop Proposal Text</h2>
      <p class="sub">Content below is placed as-is from your PDF.</p>
      <div class="card">
        <pre class="pdftext">Workshop Proposal Submission at CVPR 2026
WORKSHOP ON ANY-TO-ANY MULTIMODAL LEARNING
Shengqiong Wu1
, Wei Dai2
, Han Lin3
, Chenyu (Monica) Wang2
,
Yichen Li2
, Sharut Gupta2
, Roman Bachmann4
, Elisa Ricci5
1National University of Singapore, 2MIT, 3University of North Carolina at Chapel Hill,
4Swiss Federal Institute of Technology (EPFL), 5University of Trento
ABSTRACT
The recent surge of multimodal large models has brought unprecedented progress in connecting
language, vision, audio, and beyond. Yet, despite their impressive performance, most existing
systems remain constrained to fixed modality pairs, lacking the flexibility to generalize or reason
across arbitrary modality combinations. This limitation stands in contrast to human cognition,
which fluidly integrates diverse sensory channels to build a coherent understanding of the world.
The Any-to-Any Multimodal Learning (A2A-MML) workshop aims to explore the next
frontier of multimodal intelligence—developing systems capable of understanding, aligning,
transforming, and generating across any set of modalities. We organize the discussion around
three foundational pillars: (1) Multimodal representation learning, which seeks to learn
generalizable and disentangled representations that capture both shared and modality-specific
information; (2) Multimodal transformation, which investigates architectures and principles
enabling seamless translation, fusion, and adaptation across heterogeneous modalities; and (3)
Multimodal collaboration, which studies how modalities cooperate, complement, or conflict
with each other to achieve coherent reasoning and world modeling. By bringing together
researchers from vision, language, audio, 3D, robotics, and cognitive science, this workshop
aims to establish a roadmap toward any-to-any multimodal intelligence, where machines can
perceive, reason, and communicate across modalities as fluidly as humans do.
1 WORKSHOP SUMMARY
1.1 BACKGROUND AND MOTIVATION
Multimodal learning has rapidly evolved from early dual-modality systems (e.g., text–image) to
powerful multimodal foundation models that integrate vision, language, and audio. However, the
community is now facing a crucial inflection point. Despite significant progress, today’s models
still operate under narrow and rigid modality pairings, limited supervision settings, and task-specific
adaptation schemes. This rigidity prevents them from achieving the combinatorial generalization that
underlies human-like perception and reasoning. Humans, by contrast, seamlessly integrate multiple
sensory modalities—sight, sound, touch, spatial awareness, and language—into coherent, context￾aware understanding. Realizing this capability in AI requires a paradigm shift from “multi-to-one” or
“one-to-one” fusion toward any-to-any multimodal learning.
The A2A-MML workshop is motivated by three core needs currently underserved in the community:
1) Mechanistic and representational understanding, 2) Broader modality coverage and flexible
interaction, and 3) Methodological openness beyond foundation models. In uniting these perspectives,
A2A-MML aims to serve as a cross-disciplinary bridge, linking representation learning, cognitive
modeling, embodied AI, and generative modeling. The workshop will feature keynote talks, paper
presentations, and panel discussions that collectively address one central question: How can we move
from isolated multimodal systems to a universal, any-to-any multimodal learner that understands
and generates across all sensory forms of the world?
Tagline. Bridging Representation, Transformation, and Collaboration Toward Any-to-Any Multi￾modal Intelligence
Workshop Acronym. A2A-MML: Any-to-Any Multimodal Learning

Workshop Proposal Submission at CVPR 2026
1.2 TOPICS AND THEMES
We welcome all relevant submissions in the area of multimodal learning, with particular emphasis on
emerging advances and open challenges in any-to-any multimodal intelligence, such as:
1) Multimodal Representation Learning. Multimodal joint representation (Wang et al., 2024b;
Girdhar et al., 2023; Zhu et al., 2023; Wang et al., 2024a), which seeks to relate and decompose
information inherent in multiple modalities, forms the foundation of current multimodal retrieval (Liu
et al., 2025), understanding (Han et al., 2023), and generation (Wu et al., 2024) pipelines. We
invite contributions that explore how to disentangle modality-specific factors from modality-invariant
information, learn generalizable alignment spaces under weak or unpaired supervision, and enhance
interpretability, robustness, and compositionality in multimodal representations. Works that connect
representation learning with grounded reasoning or unified architectures are particularly encouraged.
2) Multimodal Transformation. Multimodal transformation focuses on how information can be
transferred, translated, or generated across heterogeneous modalities, such as text-to-image (Li
et al., 2019; Peebles & Xie, 2023), image-to-video (Hu, 2024; Ni et al., 2023), video-to-audio (Liu
et al., 2024; Luo et al., 2023), or beyond (Mizrahi et al., 2023; Jakubik et al., 2025; Parker et al.,
2025). We welcome submissions that investigate mechanisms enabling smooth modality conversion
and adaptation, including shared latent spaces and the diffusion transformer framework. We also
encourage studies on asymmetric and zero-shot modality transfer, bidirectional generation, and
the theoretical understanding of modality transformation under limited or noisy supervision, or its
emergent capabilities.
3) Multimodal Synergtic Collaboration. True multimodal intelligence requires more than alignment,
it demands collaboration among modalities (Tang et al., 2025; Yin et al., 2025; Fei et al., 2022). This
theme emphasizes how modalities interact, complement, and compensate for one another during per￾ception and reasoning. We invite research on multimodal synergy mechanisms such as collaborative
attention, cross-modal feedback loops, co-learning paradigms, and compositional reasoning across
modalities. Topics of interest include discovering when and how modalities cooperate or conflict, and
designing architectures that explicitly model modality interaction and synergy.
4) Benchmarking and Evaluation for Any-to-Any Multimodal learning. To measure progress
toward general multimodal intelligence, robust and extensible evaluation protocols are essential (Su
et al., 2021; Liang et al., 2021; Kil et al., 2024; Fei et al., 2025). We encourage works that develop
benchmarks or diagnostic tasks evaluating arbitrary modality combinations, multimodal generaliza￾tion, and transformation fidelity. We also welcome new evaluation metrics for alignment quality,
interpretability, synergy, and reasoning consistency. Studies analyzing failure cases, dataset biases, or
modality imbalance in current multimodal models are highly relevant.
Other topics of interest include, but are not limited to: (i) unified multimodal foundation and
agentic models, (ii) representation learning for embodied and interactive systems, (iii) integration of
underexplored modalities (e.g., 3D, tactile, temporal, physiological, or environmental data), and (iv)
theoretical and cognitive perspectives on multimodal perception and reasoning.
1.3 BROADER IMPACT STATEMENT
This workshop aims to cultivate a deeper and more unified understanding of multimodal intelli￾gence, beyond the current siloed vision–language paradigm. By emphasizing the mechanisms behind
alignment, transformation, and collaboration, we hope to inspire research that is not only more
generalizable and interpretable but also more grounded in real-world multimodal diversity. En￾couraging contributions from interdisciplinary communities (e.g., vision, language, audio, robotics,
neuroscience, and cognitive science), this workshop will help bridge representation and reasoning
across modalities, paving the way for systems that can interact naturally with humans and the physical
world. We envision that fostering such understanding will advance trustworthy AI systems capable of
reasoning, adapting, and communicating seamlessly across modalities and contexts.
1.4 RELATIONSHIP TO PREVIOUS WORKSHOPS
Several recent workshops have explored themes relevant to our focus, including the BEAM 2025:
Benchmarking and Expanding AI Multimodal Approaches-CVPR25, Emergent Visual Abilities and
Limits of Foundation Models (EVAL-FoMo 2) - CVPR25, What is Next in Multimodal Foundation
Models? - CVPR25, Workshop on Video Large Language Models - CVPR25, 8th Multimodal Learning
and Applications Workshop - CVPR25, Workshop Series on Multimodal Fact-Checking and Hate
Speech Detection - AAAI25. While these workshops have greatly advanced multimodal research from
the perspectives of model scaling, performance benchmarking, and downstream applications, our
Any-to-Any Multimodal Learning (A2A-MML) workshop is distinguished by the following aspects:
1) Emphasis on mechanistic and representational understanding. Our workshop focuses on the
“why” and “how” of modality alignment and compliment, rather than purely on performance gains.
We aim to explore questions such as: What kinds of information are shared or complementary across
modalities? How can modalities be aligned, transformed, and coordinated in different combinations?
We believe this kind of mechanistic and explanatory investigation is relatively underrepresented in
existing workshops.
3) Broader modality coverage and flexible modality interaction. We explicitly encourage research
that goes beyond the typical vision–language (or vision–language–audio/video) settings, and welcome
work involving sensors, 3D data, haptics, temporal signals, physiological or medical signals, and
more. In particular, we want to encourage researchers to design alignment, transformation, and
interaction mechanisms across arbitrary combinations of modalities.
3) Methodological openness beyond foundation models. While foundation or pre-trained models
are welcome, our workshop embraces methodological diversity. We encourage contributions that ex￾plore alternative paradigms, such as self-supervised, contrastive, reinforcement, or modular learning,
for understanding the principles of multimodal representation and interaction. This openness enables
A2A-ML to serve as a unifying platform for both foundation-model researchers and those pursuing
lightweight, mechanistic, or theoretically grounded multimodal systems.
2 ORGANIZER COMMITTEE
2.1 ORGANIZER BIO
Shengqiong Wu (she/her) is a senior Ph.D. student at the National University of Singapore, under
the supervision of Prof. Tat-Seng Chua. Her research interests include multimodal learning and
large vision-language foundation models. She has been recognized with several prestigious awards,
including the Google Ph.D. Fellowship, Baidu Scholarship, and Bytedance Scholarship. She has
published over 10 papers in top-tier conferences, e.g., ICLR, ICML, NeurIPS, and CVPR. She has
co-organized multiple workshops and grand challenges, such as ACM MM and WACV.
Ñ https://sqwu.top/ # swu@u.nus.edu ³ Scholar
Wei Dai (he/him) is a Ph.D. student at MIT Media Lab under the supervision of Prof. Paul Liang.
His research interests lie in the field of multimodal learning, particularly multimodal LLMs,
and their applications to healthcare. Before joining MIT, he was a master’s student at Stanford
University, supervised by Prof. Li Fei-Fei and Prof. Ehsan Adeli. His work has been published at top
conferences, including NeurIPS, ICML, and MICCAI. His research is supported by NSF Graduate
Research Fellowships (GRFP) and ORCD Seed Fund.
Ñ https://dd.works/ # dvdai@mit.edu ³ Scholar
Han Lin (he/him) is a third-year Ph.D. student at the University of North Carolina at Chapel Hill,
supervised by Prof. Mohit Bansal. His research interests include image and video generation,
multimodal learning, and LLMs. Before joining UNC, he was a master’s student at Columbia
University, supervised by Prof. Shih-Fu Chang, Prof. Matei Ciocarlie, and Prof. Shuran Song. He
has over 13 papers in top-tier conferences such as ICLR, ICML, NeurIPS, CVPR, and COLM.
Ñ https://hl-hanlin.github.io/ # hanlincs@cs.unc.edu ³ Scholar
Yichen Li (he/him) is a Ph.D. student at MIT, working with Prof. Antonio Torralba. Her research
interests lie in the field of multimodal learning and foundational learning paradigms agnostic to data
modalities. She received the Robert Shillman Graduate Fellowship and has published works at
top-tier conferences, including CVPR, ICCV, ICLR, ICML, and ECCV. She also served as a core
organizer for multiple workshops at ECCV and RSS.
Ñ https://people.csail.mit.edu/yichenl/ ³ Scholar
Chenyu (Monica) Wang (she/her) is a Ph.D. student at MIT, advised by Prof. Tommi Jaakkola.
Her research interests lie broadly in deep generative models, reinforcement learning, multi-modal
learning, and AI for science. During her PhD, she was a research intern at Meta FAIR and Genentech.
Her research is supported by The Citadel GQS PhD Fellowship. She has published work at top-tier
conferences, including NeurIPS and ICLR.
Ñ https://chenyuwang-monica.github.io/ ³ Scholar
Sharut Gupta (she/her) is a Ph.D. student at MIT, working with Prof. Phillip Isola and Prof.
Stefanie Jegelka. Prior to her PhD, she received her Bachelor’s and Master’s (Dual) degrees from
the Indian Institute of Technology Delhi (IIT Delhi). Her research interests focus on multi-modal
representation learning, robustness, and out-of-distribution generalization. Sharut is a recipient of the
MIT Presidential Fellowship and the MathWorks Engineering Fellowship, and has published work at
top-tier conferences, including NeurIPS, ICLR, and ICML.
Ñ https://www.mit.edu/˜sharut/ # sharut@mit.edu ³ Scholar
Roman Bachmann (he/him) is a Ph.D. student at EPFL, advised by Prof. Amir Zamir. Starting in
early 2026, he will join Apple as a research scientist. His research focus is on developing scalable
objectives for training any-to-any models across a large and diverse set of modalities, as well as
training flexible tokenizers for generative modeling. He has published works at top-tier conferences,
including CVPR, ICCV, ECCV, NeurIPS, ICML, and SIGGRAPH.
Ñ https://roman-bachmann.github.io/ # roman.bachmann@epfl.ch
³ Scholar
Elisa Ricci (she/her) is a Professor at the University of Trento. She is also the Coordinator of the
Doctoral Program in Information Engineering and Computer Science at Trento, a member of ELLIS,
and an IAPR Fellow. Her research spans the intersection of computer vision, deep learning, and
robotics perception, with an emphasis on domain adaptation, continual learning, and self-supervised
learning from visual and multimodal data.
Ñ https://eliricci.eu/ ³ Scholar
Our organizing committee brings exceptional qualifications to lead this workshop through their exten￾sive conference organization experience and technical expertise. Committee members have served
in leadership roles for premier conferences, including CVPR, ICCV, ECCV, and ACM Multimedia,
while their research contributions span all workshop themes with numerous publications in top venues.
The team’s expertise covers multimodal representation learning, multimodal understanding, and
content generation, with members serving as top-tier conference area chairs, research center leaders,
and chairs of multiple technical sessions and conferences. Most importantly, many of the committee
members have experience organizing workshops and tutorials.
2.2 INVITED SPEAKERS
This workshop will feature senior scholars with extensive expertise in multimodal learning, who will
deliver keynote presentations and offer attendees valuable insights into the latest advancements in
this field. The invited speakers are as follows:
Paul Liang (he/him, MIT). Paul Pu Liang is an Assistant Professor at the MIT Media Lab and MIT
EECS, and leads the Multisensory Intelligence research group, where he investigates how machines
can learn from and reason across diverse sensory modalities to engage with the real world. He
received his PhD in Machine Learning from Carnegie Mellon University. His research focuses on
multimodal representation learning, modality transformation via unified architectures, and interactive
systems that bridge human and machine experience. He is dedicated to mentoring the next generation
of researchers and promoting broader participation in AI.
Manling Li (she/her, Northwestern University). Manling Li is an Assistant Professor of Computer
Science at Northwestern University, where she leads the Machine Learning and Language (MLL)

Workshop Proposal Submission at CVPR 2026
Lab. Her research explores how machines can reason, plan, and act across modalities—bridging
language, vision, audio, robotics, and embodied interaction—to build trustworthy, compositional, and
long-horizon intelligent systems. She holds a PhD from the University of Illinois Urbana-Champaign
and was a post-doctoral researcher at Stanford University. Prof. Manling has received many awards,
including the MIT Technology Review 35 Under 35, Microsoft Research PhD Fellowship, and the
ACL 2024 Outstanding Paper Award.
Mohit Bansal (he/him, University of North Carolina, Chapel Hill). Mohit Bansal is the John R. &
Louise S. Parker Distinguished Professor and the Director of the MURGe-Lab (UNC-NLP Group) in
the Computer Science department at UNC Chapel Hill. His research expertise is in natural language
processing and multimodal machine learning, with a particular focus on multimodal generative
models, grounded and embodied semantics, and interpretable, efficient, and generalizable deep
learning. He is an AAAI Fellow and recipient of the Presidential Early Career Award for Scientists
and Engineers (PECASE), IIT Kanpur Young Alumnus Award, and outstanding paper awards at ACL,
CVPR, and TMLR. His service includes EMNLP and CoNLL Program Co-Chair, ACM Doctoral
Dissertation Award Committee, and Associate/Action Editor for TACL, CL, and IEEE/ACM.
Zhedong Zheng (he/him, University of Macau). Zhedong Zheng is an Assistant Professor with
the University of Macau. He received the Ph.D. degree from the University of Technology Sydney
in 2021 and the B.S. degree from Fudan University in 2016. He was a postdoctoral research fellow
at the School of Computing, National University of Singapore. He received the IEEE Circuits and
Systems Society Outstanding Young Author Award of 2021. His research interests include AIGC,
Data-centric AI, and Spatial Intelligence. He actively serves the academic community, acting as a
Senior PC for IJCAI and AAAI, an Area Chair for ACM MM’24, ACM MM’25 and ICASSP’25,
and the Publication Chair for ACM MM’25 and AVSS’25.
Yossi Gandelsman (he/him, Reve). Yossi Gandelsman is a research scientist at Reve. Starting Fall
2026, he will join the Toyota Technological Institute at Chicago (TTIC) as an assistant professor.
His research interests focus on interpretability and model mechanisms across computer vision and
language. He received his PhD from the University of California, Berkeley, worked on the Perception
Team at Google DeepMind, and holds an M.Sc. from the Weizmann Institute of Science.
Georgia Gkioxari (she/her, Caltech). Georgia Gkioxari is an Assistant Professor in the Computing
& Mathematical Sciences department at Caltech. Previously, she worked as a research scientist
at Meta FAIR. She obtained her PhD under Jitendra Malik at UC Berkeley, and completed her
undergraduate studies in Greece at the National Technical University of Athens. Her research focuses
on advanced visual perception, including 2D & 3D spatial representation and reasoning, transforming
images into structured multi-modal outputs. She has received numerous honors, including the PAMI
Young Researcher Award, Google Faculty Award, and Okawa Research Award.
Saining Xie (he/him, NYU). Saining Xie is an Assistant Professor of Computer Science at NYU’s
Courant Institute and a member of the NYU Center for Data Science. He earned his Ph.D. in
Computer Science & Engineering from UC San Diego and previously worked as a research scientist
at Facebook AI Research. His research develops robust, scalable visual-intelligence systems which
bridge visual perception, representation learning, and commonsense reasoning.
2.3 DIVERSITY OF ORGANIZING TEAM AND SPEAKERS
Our organizing committee and invited speakers represent diversity across multiple dimensions,
including gender, geography, career stages, and academic/industry backgrounds. The committee
includes representatives from institutions in Asia, the USA, and Europe, with a balance of senior
professors and senior student researchers. Our invited speakers represent diverse perspectives from
both academia and industry across multiple continents, from Asia to the USA. We are committed to
ensuring diversity in paper selection and will actively encourage submissions from underrepresented
groups through our promotion strategy.
3 WORKSHOP FORMAT AND CONTENT
3.1 TENTATIVE SCHEDULE
We plan to host a hybrid workshop format, accommodating both onsite and online participation.
The program will consist of three main components: (1) Invited keynote talks, (2) Oral and poster

Workshop Proposal Submission at CVPR 2026
Table 1: Workshop Schedule
Time Schedule Speaker
• Morning Schedule
08:50 – 09:00 Introduction and opening remarks -
09:00 – 09:30 Keynote Talk 1 TBD
09:30 – 10:00 Keynote Talk 2 TBD
10:00 – 10:40 Oral Presentations -
10:40 – 11:00 Coffee Break -
11:00 – 11:30 Keynote Talk 3 TBD
11:30 – 12:00 Keynote Talk 4 TBD
12:00 – 12:30 Poster Session 1 (Interactive) + Virtual Gallery
12:30 – 13:30 Lunch Break
• Afternoon Schedule
13:30 – 14:00 Keynote Talk 5 TBD
14:00 – 14:30 Keynote Talk 6 TBD
14:30 – 15:20 Poster Session 2 (Interactive) + Virtual Gallery -
15:20 – 15:50 Coffee Break -
15:50 – 16:20 Keynote Talk 7 TBD
16:20 – 17:20 Panel Discussion TBD
17:20 – 17:30 Closing Remarks + Best Paper Award TBD
presentations of accepted papers, and (3) A closing panel discussion featuring invited speakers,
focusing on future directions and open challenges in unified multimodal intelligence. Poster sessions
will be conducted onsite with dedicated time for interactive discussions. For remote attendees, we
will offer a virtual poster gallery and live Q&A channels to ensure inclusive engagement.
Given the rapidly growing importance of multimodal understanding as a foundation for artificial
general intelligence (AGI), particularly for world modeling, embodied reasoning, and robotic con￾struction, we propose to organize this workshop as a full-day event. Despite recent advances in
integrating vision, language, audio, and other modalities, fundamental challenges remain unsolved,
including how to achieve efficient transformation, alignment, and collaboration across heterogeneous
modalities. Addressing these issues is critical for advancing unified and interpretable multimodal
intelligence. Our A2A-MML workshop directly targets these questions and has attracted several
confirmed and tentative speakers who are leading researchers in multimodal representation learning,
cross-modal reasoning, and interactive intelligence. A full-day format will allow sufficient time for
keynote talks, oral and poster sessions, and in-depth discussions, facilitating meaningful interactions
and cross-domain exchange. The detailed schedule is provided in Table 1.
Important Dates for Review Process. We will follow the suggested dates by CVPR.
• Workshop paper submission deadline: March 20, 2026.
• Workshop paper notification date: April 5, 2026
• Final workshop program, camera-ready, videos uploaded: April 12, 2026
3.2 TARGET AUDIENCE AND ANTICIPATED AUDIENCE SIZE
This workshop is intended for three main groups: (1) Academic researchers, including professors
and graduate students working on multimodal intelligence, multimedia analysis, and cross-modal
learning; (2) Industry practitioners, specifically data scientists and engineers developing multimodal
applications; and (3) Broader participants from startups, government agencies, and interdisciplinary
domains interested in latest multimodal AI advances. We plan a hybrid format, with the majority of
participants attending in person. We expect around 100-300 on-site and 80+ virtual participants.
3.3 PAPER SUBMISSION AND ACCEPTANCE
We welcome technical, position, or perspective papers related to the topics outlined in Section 1.2.
All submissions must be written in English, follow the official CVPR proceedings format, and adhere
to the double-blind review policy.
• Tiny or Short Papers (2–4 pages) — We invite concise papers that present implementations and
evaluations of unpublished but insightful ideas, moderate yet self-contained theoretical analyses,
follow-up experiments, re-analyses of prior work, or new perspectives on existing research.
• Regular Papers (up to 8 pages, including figures and tables) — We encourage submissions
introducing original methods (e.g., architectures, training paradigms), novel research visions,
applications, or discussions of open challenges in multimodal learning.
We plan to accept approximately 25 papers in total. All accepted papers will be presented as
posters during the workshop, and 6–8 of them will be selected for short oral presentations. A Best
Paper Award will be presented based on reviewer scores and the workshop committee’s evaluation.
3.4 REVIEW PROCESS
We will use OpenReview to organize the review cycle. Each paper will be reviewed by three PC
members. Besides, we will provide review guidelines to express our expectations for the reviews.
Authors will mark conflicts of interest with the workshop organizers and the program committee,
and the review will be handled accordingly. Organizers are allowed to submit their own works to
the workshop, as OpenReview disables the visibility of PC assignments and reviews, while another
organizer can handle the decision-making for that submission.
3.5 POTENTIAL PROGRAM COMMITTEE MEMBERS
We anticipate approximately 30–40 valid submissions. To ensure each paper receives three high￾quality reviews while limiting reviewer load to a maximum of 3 papers. To oversee the review
process, we will appoint a small number of Senior Program Committee (SPC) members to assist with
reviewer recruitment, assignment, and quality control. Below is a preliminary list of confirmed and
invited PC/SPC members.
Lizi Liao (SMU, Singapore), Wei Ji (Nanjing University, CN), Xintao Wang (Kuaishou Technology),
Weicai Ye (Kuaishou Technology), Kevin Lin (Microsoft Azure AI, USA), Giorgos Tolias (CTU,
Czech), Xiangtai Li (NTU, Singapore), Bo Han (Hong Kong Baptist University, CN HK), Efstratios
Gavves (UvA, Netherlands), Xiatian Zhu (University of Surrey, UK), Juncheng Li (Zhejiang Uni￾versity, CN), Long Chen (HKUST, CN HK), Xingya Du (University of Texas at Dallas, USA), Qian
Liu (University of Auckland, New Zealand), Yichen Li (MIT CSAIL, US), Boyang Deng (Stanford
University, US), Zhengfei Kuang (Stanford University, US), Xiaoxiao Sun (Stanford University, US),
Xuan Ju (CUHK, CN HK), Yu (Bryan) Zhou (UCLA, US), Shenhan Qian (Technical University of
Munich, Germany), Chengzu Li (University of Cambridge, UK), Xingyu Fu (Princeton University,
US), Daohan Lu (NYU, US), JOY HSU (Stanford University, US), DaoAn Zhang (University of
Rochester, US), Jiafei Duan (University of Washington, US), Yukang Yang (Princeton University,
US), Zijian Zhou (King’s College London, UK), Jinjie Mai (KAUST, KSA), Junhao Zhang (Google,
US), Jiawei Liu (Meta FAIR, US), Zhijie Deng (Shanghai Jiao Tong University, CN), Qinghong Lin
(University of Oxford, UK), Yuchao Gu (National Univeristy of Singapore, SG).
3.6 WORKSHOP PROMOTION
The workshop will be promoted through multiple channels. A dedicated website will host detailed
information about the agenda, speakers, and registration process. In addition to the website, we will
utilize social media platforms, including X and LinkedIn, to expand our reach to a wider audience.
Organizers and speakers will be encouraged to share the event within their networks. Accepted
papers will be posted on the workshop website, with an option to publish them on arXiv, noting their
association with the event. After the workshop, we will strive to make slides and recordings of the
talks available online.
4 TECHNICAL AND LOGISTICAL REQUIREMENTS
We plan to host the workshop in a hybrid format to ensure broad accessibility. The following technical
and logistical support will be required:
• Audio/Visual Support: Projector, microphones, and recording equipment for in-person presenta￾tions and live streaming.
• Virtual Platform Access: Integration with the official CVPR virtual platform to enable remote par￾ticipation, including support for live keynote talks, oral sessions, poster sessions (e.g., GatherTown
or virtual poster gallery), and interactive Q&A.
• Poster Display: Space and equipment (e.g., poster boards or screens) for in-person poster sessions.
• Wi-Fi and Technical Staff: Reliable high-speed internet and on-site staff support to manage
transitions between in-person and virtual components.
REFERENCES
Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting
Wang, Junbao Zhou, Jiahao Meng, et al. On path to multimodal generalist: General-level and
general-bench. In Forty-second International Conference on Machine Learning, 2025.
Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jingyuan Wen, Haoyu Lu, Ruihua Song,
Xin Gao, Tao Xiang, et al. Towards artificial general intelligence via a multimodal foundation
model. Nature Communications, 13(1):3094, 2022.
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand
Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pp. 15180–15190, 2023.
Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu,
Song Wen, Ziyu Guo, et al. Imagebind-llm: Multi-modality instruction tuning. arXiv preprint
arXiv:2309.03905, 2023.
Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
8153–8163, 2024.
Johannes Jakubik, Felix Yang, Benedikt Blumenstiel, Erik Scheurer, Rocco Sedona, Stefano Mauro￾giovanni, Jente Bosmans, Nikolaos Dionelis, Valerio Marsocci, Niklas Kopp, Rahul Ramachan￾dran, Paolo Fraccaro, Thomas Brunschwiler, Gabriele Cavallaro, Juan Bernabe-Moreno, and ´
Nicolas Long’ep’e. Terramind: Large-scale generative multimodality for earth observation.
ArXiv, abs/2504.11171, 2025. URL https://api.semanticscholar.org/CorpusID:
277786839.
Jihyung Kil, Zheda Mai, Justin Lee, Arpita Chowdhury, Zihe Wang, Kerrie Cheng, Lemeng Wang,
Ye Liu, and Wei-Lun Harry Chao. Mllm-compbench: A comparative reasoning benchmark for
multimodal llms. Advances in Neural Information Processing Systems, 37:28798–28827, 2024.
Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip Torr. Controllable text-to-image generation.
Advances in neural information processing systems, 32, 2019.
Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu,
Michelle A Lee, Yuke Zhu, et al. Multibench: Multiscale benchmarks for multimodal representation
learning. Advances in neural information processing systems, 2021(DB1):1, 2021.
Xiaohao Liu, Xiaobo Xia, See-Kiong Ng, and Tat-Seng Chua. Principled multimodal representation
learning. arXiv preprint arXiv:2507.17343, 2025.
Xiulong Liu, Kun Su, and Eli Shlizerman. Tell what you hear from what you see-video to audio
generation through text. Advances in Neural Information Processing Systems, 37:101337–101366,
2024.
Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio
synthesis with latent diffusion models. Advances in Neural Information Processing Systems, 36:
48855–48876, 2023.
David Mizrahi, Roman Bachmann, Oguzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, ˘
and Amir Zamir. 4M: Massively multimodal masked modeling. In Advances in Neural Information
Processing Systems, 2023.
Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and Martin Renqiang Min. Conditional
image-to-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pp. 18444–18455, 2023.
Liam Holden Parker, Franc¸ois Lanusse, Jeff Shen, Ollie Liu, Tom Hehir, Leopoldo Sarra, Lucas
Meyer, Micah Bowles, Sebastian Wagner-Carena, Helen Qu, Siavash Golkar, Alberto Bietti, Hatim
Bourfoune, Nathan Casserau, Pierre Cornette, Keiya Hirashima, Geraud Krawezik, Ruben Ohana, ´
Nicholas Lourie, Michael McCabe, Rudy Morel, Payel Mukhopadhyay, Mariel Pettee, Bruno
Regaldo-Saint Blancard, Kyunghyun Cho, Miles Cranmer, and Shirley Ho. Aion-1: Omnimodal ´
foundation model for astronomical sciences. 2025. URL https://api.semanticscholar.
org/CorpusID:282246063.
William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of
the IEEE/CVF international conference on computer vision, pp. 4195–4205, 2023.
Lin Su, Nan Duan, Edward Cui, Lei Ji, Chenfei Wu, Huaishao Luo, Yongfei Liu, Ming Zhong, Taroon
Bharti, and Arun Sacheti. Gem: A general evaluation benchmark for multimodal tasks. arXiv
preprint arXiv:2106.09889, 2021.
Hao Tang, Shengfeng He, and Jing Qin. Connecting giants: Synergistic knowledge transfer of large
multimodal models for few-shot learning. arXiv preprint arXiv:2510.11115, 2025.
Chenyu Wang, Sharut Gupta, Xinyi Zhang, Sana Tonekaboni, Stefanie Jegelka, Tommi Jaakkola, and
Caroline Uhler. An information criterion for controlled disentanglement of multimodal data. arXiv
preprint arXiv:2410.23996, 2024a.
Zehan Wang, Ziang Zhang, Hang Zhang, Luping Liu, Rongjie Huang, Xize Cheng, Hengshuang
Zhao, and Zhou Zhao. Omnibind: Large-scale omni multimodal representation via binding spaces.
arXiv preprint arXiv:2407.11895, 2024b.
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal
llm. In Forty-first International Conference on Machine Learning, 2024.
Xiaotian Yin, Xin Liu, Si Chen, Yuan Wang, Yuwen Pan, and Tianzhu Zhang. Exploring the better
multimodal synergy strategy for vision-language models. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 39, pp. 22182–22190, 2025.
Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang,
Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n￾modality by language-based semantic alignment. arXiv preprint arXiv:2310.01852, 2023.</pre>
      </div>
    </div>
  </section>

  <section id="organizers">
    <div class="container">
      <h2 class="section-title">Organizers</h2>
      <p class="sub">Text is included in the proposal section above.</p>
    </div>
  </section>

  <section id="format">
    <div class="container">
      <h2 class="section-title">Workshop Format</h2>
      <p class="sub">Text is included in the proposal section above.</p>
    </div>
  </section>

  <section id="references">
    <div class="container">
      <h2 class="section-title">References</h2>
      <p class="sub">Text is included in the proposal section above.</p>
    </div>
  </section>

  <footer>
    <div class="container">
      <div class="muted">CVPR 2026 Workshop on Any-to-Any Multimodal Learning</div>
      <div class="muted">Theme: paper · ink · gunn red</div>
    </div>
  </footer>
</body>
</html>
