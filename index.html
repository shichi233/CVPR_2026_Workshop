<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>CVPR 2026 Workshop on Any-to-Any Multimodal Learning</title>
  <style>
    :root{
      --primary:#9b0d13;
      --bg:#ffffff;
      --card:#ffffff;
      --text:#000000;
      --muted:#444444;
      --line:#dddddd;
      --radius:12px;
      --max:1100px;
    }

    *{ box-sizing:border-box }

    body{
      margin:0;
      font:16px/1.7 system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;
      background:var(--bg);
      color:var(--text);
    }

    .container{
      max-width:var(--max);
      margin:0 auto;
      padding:32px 20px;
    }

    header{
      border-bottom:2px solid var(--primary);
      background:#ffffff;
    }

    h1,h2,h3{
      line-height:1.25;
      color:#000000;
    }

    h1{
      font-size:36px;
      margin:0 0 12px;
    }

    h2{
      font-size:26px;
      margin:36px 0 14px;
      color:var(--primary);
    }

    h3{
      font-size:19px;
      margin:26px 0 10px;
    }

    p{
      margin:10px 0;
      color:var(--text);
    }

    ul{
      margin:10px 0 10px 24px;
    }

    li{
      margin:6px 0;
    }

    .card{
      background:var(--card);
      border:1px solid var(--line);
      border-radius:var(--radius);
      padding:24px;
      margin:22px 0;
    }

    .tag{
      display:inline-block;
      padding:6px 12px;
      border-radius:999px;
      background:#ffffff;
      border:1.5px solid var(--primary);
      color:var(--primary);
      font-weight:600;
      margin-bottom:14px;
      font-size:14px;
    }

    .section{
      margin-top:44px;
    }

    footer{
      border-top:1px solid var(--line);
      margin-top:60px;
      padding:28px 20px;
      color:var(--muted);
      font-size:14px;
    }
  </style>
</head>

<body>
<header>
  <div class="container">
    <span class="tag">CVPR 2026 Workshop Proposal</span>
    <h1>WORKSHOP ON ANY-TO-ANY MULTIMODAL LEARNING</h1>
    <p>
      Shengqiong Wu, Wei Dai, Han Lin, Chenyu (Monica) Wang, Yichen Li, Sharut Gupta,
      Roman Bachmann, Elisa Ricci
    </p>
    <p>
      National University of Singapore, MIT, University of North Carolina at Chapel Hill,
      Swiss Federal Institute of Technology (EPFL), University of Trento
    </p>
  </div>
</header>

<main class="container">

<div class="section card">
<h2>ABSTRACT</h2>
<p>
The recent surge of multimodal large models has brought unprecedented progress in connecting
language, vision, audio, and beyond. Yet, despite their impressive performance, most existing
systems remain constrained to fixed modality pairs, lacking the flexibility to generalize or reason
across arbitrary modality combinations. This limitation stands in contrast to human cognition,
which fluidly integrates diverse sensory channels to build a coherent understanding of the world.
The Any-to-Any Multimodal Learning (A2A-MML) workshop aims to explore the next
frontier of multimodal intelligence—developing systems capable of understanding, aligning,
transforming, and generating across any set of modalities.
</p>
</div>

<!-- 下面正文保持你现在的内容即可，不需要再改 -->

</main>

<footer>
  <div class="container">
    CVPR 2026 Workshop on Any-to-Any Multimodal Learning
  </div>
</footer>
</body>
</html>
