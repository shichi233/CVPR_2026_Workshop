<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>CVPR 2026 Workshop on Any-to-Any Multimodal Learning</title>
  <meta name="description" content="WORKSHOP ON ANY-TO-ANY MULTIMODAL LEARNING A2A-MML" />
  <style>
    :root{
      --gunn-red:#9b0d13;
      --gunn-red-dark:#6f0a0f;
      --paper:#f6f3ef;
      --ink:#0f0f12;
      --muted:#5e5a55;
      --line:#e9e4dc;
      --card:#ffffff;
      --radius:18px;
      --shadow:0 8px 24px rgba(0,0,0,.10);
      --max:1100px;
    }

    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0;
      font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,"Helvetica Neue",Arial,"Noto Sans","Apple Color Emoji","Segoe UI Emoji";
      background:var(--paper);
      color:var(--ink);
      line-height:1.55;
    }
    a{color:inherit;text-decoration:none}
    .container{max-width:var(--max);margin:0 auto;padding:0 20px}

    .nav{
      position:sticky;
      top:0;
      z-index:50;
      backdrop-filter:saturate(1.2) blur(6px);
      background:rgba(246,243,239,.78);
      border-bottom:1px solid var(--line);
    }
    .nav-inner{
      display:flex;
      align-items:center;
      justify-content:space-between;
      gap:14px;
      padding:10px 0;
    }
    .brand{
      display:flex;
      align-items:center;
      gap:12px;
      font-weight:900;
      letter-spacing:.2px;
      white-space:nowrap;
    }
    .mark{
      width:40px;
      height:40px;
      border-radius:12px;
      background:linear-gradient(135deg,var(--gunn-red),var(--gunn-red-dark));
      box-shadow:0 2px 10px rgba(0,0,0,.12);
    }
    .menu{
      display:flex;
      align-items:center;
      gap:16px;
      flex-wrap:wrap;
      font-weight:650;
    }
    .menu a{
      opacity:.85;
      text-underline-offset:6px;
    }
    .menu a:hover{opacity:1;text-decoration:underline}

    .hero{
      background:var(--gunn-red);
      color:#fff;
    }
    .hero .wrap{
      padding:52px 20px 58px;
    }
    .badge{
      display:inline-flex;
      align-items:center;
      gap:10px;
      padding:8px 14px;
      border-radius:999px;
      background:rgba(255,255,255,.12);
      border:1px solid rgba(255,255,255,.25);
      font-weight:800;
    }
    .hero h1{
      margin:14px 0 10px;
      font-size:clamp(28px,5.6vw,52px);
      line-height:1.06;
      letter-spacing:.2px;
    }
    .hero p{
      margin:8px 0 0;
      opacity:.95;
      font-size:clamp(15px,1.6vw,18px);
      max-width:72ch;
    }

    section{padding:52px 0}
    .section-title{
      margin:0 0 12px;
      font-size:clamp(22px,3.2vw,34px);
      color:var(--ink);
      letter-spacing:.2px;
    }

    .box{
      background:var(--card);
      border:1px solid var(--line);
      border-radius:var(--radius);
      padding:22px;
      box-shadow:var(--shadow);
    }

    .content h2{
      margin: 0 0 14px;
      font-size: 30px;
      color: var(--gunn-red);
      font-weight: 900;
      letter-spacing: .2px;
    }
    .content h3{
      margin: 22px 0 10px;
      font-size: 18px;
      font-weight: 900;
      color: var(--ink);
      letter-spacing: .2px;
    }
    .content h3::before{
      content: "";
      display: block;
      height: 1px;
      background: var(--line);
      margin-bottom: 14px;
    }
    .content h4{
      margin: 18px 0 10px;
      font-size: 16px;
      font-weight: 900;
      color: var(--ink);
      letter-spacing: .2px;
    }
    .content p{
      margin: 10px 0;
    }
    .content ul{
      margin: 10px 0 10px 22px;
    }
    .content li{
      margin: 6px 0;
    }

    footer{
      background:#111;
      color:#fff;
      margin-top:40px;
    }
    footer .container{
      padding:26px 20px;
      display:flex;
      justify-content:space-between;
      align-items:center;
      gap:16px;
      flex-wrap:wrap;
    }
    footer .muted{opacity:.85;font-weight:650}

    @media (max-width: 900px){
      .menu{gap:12px}
    }
  </style>
</head>

<body>
  <nav class="nav" aria-label="Primary">
    <div class="container nav-inner">
      <a class="brand" href="#top">
        <span class="mark" aria-hidden="true"></span>
        <span>A2A-MML · CVPR 2026</span>
      </a>
      <div class="menu">
        <a href="#abstract">Abstract</a>
        <a href="#workshop-summary">Workshop Summary</a>
        <a href="#organizers">Organizers</a>
        <a href="#format">Workshop Format</a>
        <a href="#references">References</a>
      </div>
    </div>
  </nav>

  <header id="top" class="hero">
    <div class="container wrap">
      <span class="badge">WORKSHOP ON ANY-TO-ANY MULTIMODAL LEARNING</span>
      <h1>CVPR 2026 Workshop</h1>
      <p>Shengqiong Wu, Wei Dai, Han Lin, Chenyu Monica Wang, Yichen Li, Sharut Gupta, Roman Bachmann, Elisa Ricci</p>
    </div>
  </header>

  <main class="content">

    <section id="abstract">
      <div class="container">
        <h2 class="section-title">Abstract</h2>
        <div class="box">
          <p>The recent surge of multimodal large models has brought unprecedented progress in connecting language, vision, audio, and beyond. Yet, despite their impressive performance, most existing systems remain constrained to fixed modality pairs, lacking the flexibility to generalize or reason across arbitrary modality combinations. This limitation stands in contrast to human cognition, which fluidly integrates diverse sensory channels to build a coherent understanding of the world. The Any-to-Any Multimodal Learning (A2A-MML) workshop aims to explore the next frontier of multimodal intelligence—developing systems capable of understanding, aligning, transforming, and generating across any set of modalities. We organize the discussion around three foundational pillars: (1) Multimodal representation learning, which seeks to learn generalizable and disentangled representations that capture both shared and modality-specific information; (2) Multimodal transformation, which investigates architectures and principles enabling seamless translation, fusion, and adaptation across heterogeneous modalities; and (3) Multimodal collaboration, which studies how modalities cooperate, complement, or conflict with each other to achieve coherent reasoning and world modeling. By bringing together researchers from vision, language, audio, 3D, robotics, and cognitive science, this workshop aims to establish a roadmap toward any-to-any multimodal intelligence, where machines can perceive, reason, and communicate across modalities as fluidly as humans do.</p>
        </div>
      </div>
    </section>

    <section id="workshop-summary">
      <div class="container">
        <h2 class="section-title">Workshop Summary</h2>

        <h3>Background and Motivation</h3>
        <div class="box">
          <p>Multimodal learning has rapidly evolved from early dual-modality systems (e.g., text–image) to powerful multimodal foundation models that integrate vision, language, and audio. However, the community is now facing a crucial inflection point. Despite significant progress, today’s models still operate under narrow and rigid modality pairings, limited supervision settings, and task-specific adaptation schemes. This rigidity prevents them from achieving the combinatorial generalization that underlies human-like perception and reasoning. Humans, by contrast, seamlessly integrate multiple sensory modalities—sight, sound, touch, spatial awareness, and language—into coherent, context-aware understanding. Realizing this capability in AI requires a paradigm shift from “multi-to-one” or “one-to-one” fusion toward any-to-any multimodal learning.</p>
          <p>The A2A-MML workshop is motivated by three core needs currently underserved in the community: 1) Mechanistic and representational understanding, 2) Broader modality coverage and flexible interaction, and 3) Methodological openness beyond foundation models. In uniting these perspectives, A2A-MML aims to serve as a cross-disciplinary bridge, linking representation learning, cognitive modeling, embodied AI, and generative modeling. The workshop will feature keynote talks, paper presentations, and panel discussions that collectively address one central question: How can we move from isolated multimodal systems to a universal, any-to-any multimodal learner that understands and generates across all sensory forms of the world?</p>
          <p><strong>Tagline.</strong> Bridging Representation, Transformation, and Collaboration Toward Any-to-Any Multimodal Intelligence</p>
          <p><strong>Workshop Acronym.</strong> A2A-MML: Any-to-Any Multimodal Learning</p>
        </div>

        <h3>Topics and Themes</h3>
        <div class="box">
          <p>We welcome all relevant submissions in the area of multimodal learning, with particular emphasis on emerging advances and open challenges in any-to-any multimodal intelligence, such as:</p>

          <h4>Multimodal Representation Learning</h4>
          <p>Multimodal joint representation (Wang et al., 2024b; Girdhar et al., 2023; Zhu et al., 2023; Wang et al., 2024a), which seeks to relate and decompose information inherent in multiple modalities, forms the foundation of current multimodal retrieval (Liu et al., 2025), understanding (Han et al., 2023), and generation (Wu et al., 2024) pipelines. We invite contributions that explore how to disentangle modality-specific factors from modality-invariant information, learn generalizable alignment spaces under weak or unpaired supervision, and enhance interpretability, robustness, and compositionality in multimodal representations. Works that connect representation learning with grounded reasoning or unified architectures are particularly encouraged.</p>

          <h4>Multimodal Transformation</h4>
          <p>Multimodal transformation focuses on how information can be transferred, translated, or generated across heterogeneous modalities, such as text-to-image (Li et al., 2019; Peebles &amp; Xie, 2023), image-to-video (Hu, 2024; Ni et al., 2023), video-to-audio (Liu et al., 2024; Luo et al., 2023), or beyond (Mizrahi et al., 2023; Jakubik et al., 2025; Parker et al., 2025). We welcome submissions that investigate mechanisms enabling smooth modality conversion and adaptation, including shared latent spaces and the diffusion transformer framework. We also encourage studies on asymmetric and zero-shot modality transfer, bidirectional generation, and the theoretical understanding of modality transformation under limited or noisy supervision, or its emergent capabilities.</p>

          <h4>Multimodal Synergtic Collaboration</h4>
          <p>True multimodal intelligence requires more than alignment, it demands collaboration among modalities (Tang et al., 2025; Yin et al., 2025; Fei et al., 2022). This theme emphasizes how modalities interact, complement, and compensate for one another during perception and reasoning. We invite research on multimodal synergy mechanisms such as collaborative attention, cross-modal feedback loops, co-learning paradigms, and compositional reasoning across modalities. Topics of interest include discovering when and how modalities cooperate or conflict, and designing architectures that explicitly model modality interaction and synergy.</p>

          <h4>Benchmarking and Evaluation for Any-to-Any Multimodal learning</h4>
          <p>To measure progress toward general multimodal intelligence, robust and extensible evaluation protocols are essential (Su et al., 2021; Liang et al., 2021; Kil et al., 2024; Fei et al., 2025). We encourage works that develop benchmarks or diagnostic tasks evaluating arbitrary modality combinations, multimodal generalization, and transformation fidelity. We also welcome new evaluation metrics for alignment quality, interpretability, synergy, and reasoning consistency. Studies analyzing failure cases, dataset biases, or modality imbalance in current multimodal models are highly relevant.</p>
        </div>

        <h3>Broader Impact Statement</h3>
        <div class="box">
          <p>This workshop aims to cultivate a deeper and more unified understanding of multimodal intelligence, beyond the current siloed vision–language paradigm. By emphasizing the mechanisms behind alignment, transformation, and collaboration, we hope to inspire research that is not only more generalizable and interpretable but also more grounded in real-world multimodal diversity. Encouraging contributions from interdisciplinary communities, including vision, language, audio, robotics, neuroscience, and cognitive science, this workshop will help bridge representation and reasoning across modalities, paving the way for systems that can interact naturally with humans and the physical world. We envision that fostering such understanding will advance trustworthy AI systems capable of reasoning, adapting, and communicating seamlessly across modalities and contexts.</p>
        </div>
      </div>
    </section>

    <section id="organizers">
      <div class="container">
        <h2 class="section-title">Organizers</h2>

        <h3>Organizer Bio</h3>
        <div class="box">
          <p>Shengqiong Wu (she/her) is a senior Ph.D. student at the National University of Singapore, under the supervision of Prof. Tat-Seng Chua. Her research interests include multimodal learning and large vision-language foundation models. She has been recognized with several prestigious awards, including the Google Ph.D. Fellowship, Baidu Scholarship, and Bytedance Scholarship. She has published over 10 papers in top-tier conferences, e.g., ICLR, ICML, NeurIPS, and CVPR. She has co-organized multiple workshops and grand challenges, such as ACM MM and WACV.</p>
          <p>Wei Dai (he/him) is a Ph.D. student at MIT Media Lab under the supervision of Prof. Paul Liang. His research interests lie in the field of multimodal learning, particularly multimodal LLMs, and their applications to healthcare. Before joining MIT, he was a master’s student at Stanford University, supervised by Prof. Li Fei-Fei and Prof. Ehsan Adeli. His work has been published at top conferences, including NeurIPS, ICML, and MICCAI. His research is supported by NSF Graduate Research Fellowships and ORCD Seed Fund.</p>
          <p>Han Lin (he/him) is a third-year Ph.D. student at the University of North Carolina at Chapel Hill, supervised by Prof. Mohit Bansal. His research interests include image and video generation, multimodal learning, and LLMs. Before joining UNC, he was a master’s student at Columbia University, supervised by Prof. Shih-Fu Chang, Prof. Matei Ciocarlie, and Prof. Shuran Song. He has over 13 papers in top-tier conferences such as ICLR, ICML, NeurIPS, CVPR, and COLM.</p>
          <p>Yichen Li (he/him) is a Ph.D. student at MIT, working with Prof. Antonio Torralba. Her research interests lie in the field of multimodal learning and foundational learning paradigms agnostic to data modalities. She received the Robert Shillman Graduate Fellowship and has published works at top-tier conferences, including CVPR, ICCV, ICLR, ICML, and ECCV. She also served as a core organizer for multiple workshops at ECCV and RSS.</p>
          <p>Chenyu Monica Wang (she/her) is a Ph.D. student at MIT, advised by Prof. Tommi Jaakkola. Her research interests lie broadly in deep generative models, reinforcement learning, multi-modal learning, and AI for science. During her PhD, she was a research intern at Meta FAIR and Genentech. Her research is supported by The Citadel GQS PhD Fellowship. She has published work at top-tier conferences, including NeurIPS and ICLR.</p>
          <p>Sharut Gupta (she/her) is a Ph.D. student at MIT, working with Prof. Phillip Isola and Prof. Stefanie Jegelka. Prior to her PhD, she received her Bachelor’s and Master’s (Dual) degrees from the Indian Institute of Technology Delhi. Her research interests focus on multi-modal representation learning, robustness, and out-of-distribution generalization. Sharut is a recipient of the MIT Presidential Fellowship and the MathWorks Engineering Fellowship, and has published work at top-tier conferences, including NeurIPS, ICLR, and ICML.</p>
          <p>Roman Bachmann (he/him) is a Ph.D. student at EPFL, advised by Prof. Amir Zamir. Starting in early 2026, he will join Apple as a research scientist. His research focus is on developing scalable objectives for training any-to-any models across a large and diverse set of modalities, as well as training flexible tokenizers for generative modeling. He has published works at top-tier conferences, including CVPR, ICCV, ECCV, NeurIPS, ICML, and SIGGRAPH.</p>
          <p>Elisa Ricci (she/her) is a Professor at the University of Trento. She is also the Coordinator of the Doctoral Program in Information Engineering and Computer Science at Trento, a member of ELLIS, and an IAPR Fellow. Her research spans the intersection of computer vision, deep learning, and robotics perception, with an emphasis on domain adaptation, continual learning, and self-supervised learning from visual and multimodal data.</p>
        </div>
      </div>
    </section>

    <section id="format">
      <div class="container">
        <h2 class="section-title">Workshop Format</h2>

        <h3>Tentative Schedule</h3>
        <div class="box">
          <p>We plan to host a hybrid workshop format, accommodating both onsite and online participation.</p>
          <p><strong>Important Dates for Review Process.</strong> We will follow the suggested dates by CVPR.</p>
          <ul>
            <li>Workshop paper submission deadline: March 20, 2026.</li>
            <li>Workshop paper notification date: April 5, 2026</li>
            <li>Final workshop program, camera-ready, videos uploaded: April 12, 2026</li>
          </ul>
        </div>

        <h3>Paper Submission and Acceptance</h3>
        <div class="box">
          <p>We welcome technical, position, or perspective papers related to the topics outlined in the workshop scope. All submissions must be written in English, follow the official CVPR proceedings format, and adhere to the double-blind review policy.</p>
        </div>
      </div>
    </section>

    <section id="references">
      <div class="container">
        <h2 class="section-title">References</h2>
        <div class="box">
          <p>References are included in the proposal PDF.</p>
        </div>
      </div>
    </section>

  </main>

  <footer>
    <div class="container">
      <div class="muted">CVPR 2026 Workshop on Any-to-Any Multimodal Learning</div>
      <div class="muted">A2A-MML</div>
    </div>
  </footer>
</body>
</html>
