<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>CVPR 2026 Workshop on Any-to-Any Multimodal Learning</title>
  <meta name="description" content="WORKSHOP ON ANY-TO-ANY MULTIMODAL LEARNING A2A-MML" />
  <style>
    :root{
      --gunn-red:#9b0d13;
      --gunn-red-dark:#6f0a0f;
      --paper:#f6f3ef;
      --ink:#0f0f12;
      --muted:#5e5a55;
      --line:#e9e4dc;
      --card:#ffffff;
      --radius:18px;
      --shadow:0 10px 28px rgba(0,0,0,.10);
      --shadow-soft:0 6px 18px rgba(0,0,0,.08);
      --max:1100px;
    }

    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0;
      font-family:"Times New Roman", Times, serif;
      background:var(--paper);
      color:var(--ink);
      line-height:1.55;
    }
    a{color:inherit;text-decoration:none}
    .container{max-width:var(--max);margin:0 auto;padding:0 20px}

    .nav{
      position:sticky;
      top:0;
      z-index:50;
      backdrop-filter:saturate(1.2) blur(6px);
      background:rgba(246,243,239,.82);
      border-bottom:1px solid var(--line);
    }
    .nav-inner{
      display:flex;
      align-items:center;
      justify-content:space-between;
      gap:14px;
      padding:10px 0;
    }
    .brand{
      display:flex;
      align-items:center;
      gap:12px;
      font-weight:700;
      letter-spacing:.2px;
      white-space:nowrap;
    }
    .logo{
      width:40px;
      height:40px;
      border-radius:12px;
      object-fit:cover;
      display:block;
      box-shadow:0 2px 10px rgba(0,0,0,.12);
    }

    .menu{
      display:flex;
      align-items:center;
      gap:16px;
      flex-wrap:wrap;
      font-weight:600;
    }
    .menu a{opacity:.86;text-underline-offset:6px;}
    .menu a:hover{opacity:1;text-decoration:underline}

    .hero{
      position:relative;
      color:#fff;
      min-height:72vh;
      background:url("./background.png") center/cover no-repeat fixed;
      display:flex;
      align-items:center;
    }
    .hero::after{
      content:"";
      position:absolute;inset:0;
      background:linear-gradient(180deg, rgba(0,0,0,.45), rgba(0,0,0,.45));
    }
    .hero .wrap{
      position:relative;
      z-index:1;
      width:100%;
      padding:72px 20px 80px;
      text-align:center;
    }
    .badge{
      display:inline-flex;
      align-items:center;
      gap:10px;
      padding:10px 16px;
      border-radius:999px;
      background:rgba(255,255,255,.16);
      border:1px solid rgba(255,255,255,.28);
      font-weight:700;
      letter-spacing:.3px;
    }
    .hero h1{
      margin:16px 0 26px;
      font-size:clamp(34px,6.4vw,64px);
      line-height:1.04;
      letter-spacing:.2px;
      font-weight:700;
    }

    .cta{
      display:flex;
      justify-content:center;
      margin-top:8px;
    }
    .cta-btn{
      display:inline-flex;
      align-items:center;
      justify-content:center;
      padding:14px 22px;
      border-radius:14px;
      border:1px solid rgba(255,255,255,.22);
      background:#3b6df6;
      color:#fff;
      font-weight:700;
      box-shadow:0 12px 26px rgba(0,0,0,.18);
      transition:transform .12s ease, box-shadow .12s ease, background .12s ease;
      text-transform:capitalize;
    }
    .cta-btn:hover{
      transform:translateY(-1px);
      box-shadow:0 18px 38px rgba(0,0,0,.22);
      background:#2e58cc;
    }

    main{padding-bottom:40px}
    section{padding:52px 0}
    .section-title{
      margin:0 0 14px;
      font-size:clamp(22px,3.2vw,34px);
      color:var(--gunn-red);
      font-weight:700;
      letter-spacing:.2px;
    }

    .box{
      background:var(--card);
      border:1px solid var(--line);
      border-radius:var(--radius);
      padding:22px;
      box-shadow:var(--shadow-soft);
    }

    .content h3{
      margin:24px 0 12px;
      font-size:20px;
      font-weight:700;
      color:var(--ink);
      letter-spacing:.2px;
    }
    .content h3::before{
      content:"";
      display:block;
      height:1px;
      background:var(--line);
      margin-bottom:14px;
    }
    .content p{margin:10px 0}

    .bullets{margin:10px 0 10px 22px;padding:0;}
    .bullets li{margin:6px 0}

    .grid{
      display:grid;
      grid-template-columns:repeat(4,minmax(0,1fr));
      gap:18px;
    }
    @media (max-width: 980px){ .grid{grid-template-columns:repeat(2,minmax(0,1fr))} }
    @media (max-width: 520px){ .grid{grid-template-columns:1fr} }

    .card{
      background:var(--card);
      border:1px solid var(--line);
      border-radius:20px;
      overflow:hidden;
      box-shadow:var(--shadow-soft);
      display:flex;
      flex-direction:column;
      min-height:320px;
      cursor:pointer;
      position:relative;
      transition:transform .12s ease, box-shadow .12s ease;
    }
    .card:hover{ transform:translateY(-2px); box-shadow:var(--shadow); }
    .card .img{ aspect-ratio:4/5; background:#ddd; position:relative; overflow:hidden; }
    .card .img img{ width:100%; height:100%; object-fit:cover; display:block; }
    .card .meta{ padding:12px 14px 14px; }
    .card .name{ font-weight:700; font-size:18px; line-height:1.15; margin:0 0 6px; }
    .card .affil{ margin:0; color:var(--muted); font-weight:400; font-size:14px; line-height:1.2; }
    .card .chev{
      position:absolute; top:10px; right:10px;
      width:38px; height:38px; border-radius:999px;
      background:rgba(255,255,255,.92);
      border:1px solid rgba(0,0,0,.08);
      display:flex; align-items:center; justify-content:center;
      box-shadow:0 6px 18px rgba(0,0,0,.10);
    }
    .card .chev span{ font-size:18px; font-weight:700; color:var(--gunn-red); transform:translateX(1px); }

    .schedule{
      width:100%;
      border-collapse:separate;
      border-spacing:0;
      overflow:hidden;
      border-radius:18px;
      border:1px solid var(--line);
      background:var(--card);
      box-shadow:var(--shadow-soft);
    }
    .schedule th, .schedule td{
      text-align:left; padding:12px 12px; border-bottom:1px solid var(--line); vertical-align:top; font-weight:400;
    }
    .schedule th{ background:rgba(155,13,19,.06); color:var(--ink); font-weight:400; }
    .schedule tr:last-child td{border-bottom:none}
    .schedule .section-row td{ background:rgba(0,0,0,.03); font-weight:400; }

    .pill{ display:inline; padding:0; border:none; background:none; font-weight:400; }

    /* ===== Important Dates cards (match screenshot style) ===== */
    .dates-note{
      margin:10px 0 18px;
      color:var(--muted);
      font-weight:600;
    }

    .dates-grid{
      display:grid;
      grid-template-columns:repeat(3, minmax(0, 1fr));
      gap:22px;
      margin-top:10px;
    }
    @media (max-width: 980px){ .dates-grid{ grid-template-columns:repeat(2, minmax(0, 1fr)); } }
    @media (max-width: 520px){ .dates-grid{ grid-template-columns:1fr; } }

    .date-card{
      background:var(--card);
      border:2px solid rgba(180, 200, 235, .55);
      border-radius:22px;
      padding:22px 24px;
      box-shadow:0 10px 20px rgba(0,0,0,.10);
      min-height:170px;
      display:flex;
      flex-direction:column;
      justify-content:center;
      text-align:center;
    }
    .date-card .date{
      margin:0 0 12px;
      font-size:42px;
      font-weight:700;
      color:var(--ink);
      letter-spacing:.2px;
    }
    .date-card .label{
      margin:0;
      font-size:22px;
      color:var(--muted);
      font-weight:600;
    }

    .modal{
      position:fixed; inset:0; display:none; align-items:center; justify-content:center; padding:22px; z-index:1000; background:rgba(0,0,0,.55);
    }
    .modal.open{display:flex}
    .modal-panel{
      width:min(980px, 100%); background:var(--card); border-radius:22px; border:1px solid rgba(255,255,255,.20);
      box-shadow:0 24px 60px rgba(0,0,0,.28); overflow:hidden;
    }
    .modal-head{
      padding:18px 18px 10px; display:flex; align-items:flex-start; justify-content:space-between; gap:12px; border-bottom:1px solid var(--line);
    }
    .modal-title{ margin:0; font-weight:700; font-size:30px; color:var(--ink); letter-spacing:.2px; }
    .modal-close{
      width:44px; height:44px; border-radius:999px; border:1px solid var(--line); background:rgba(0,0,0,.02);
      cursor:pointer; display:flex; align-items:center; justify-content:center; font-weight:700; color:var(--muted);
    }
    .modal-close:hover{background:rgba(0,0,0,.05)}
    .modal-body{ padding:18px; display:grid; grid-template-columns:280px 1fr; gap:18px; }
    @media (max-width: 760px){ .modal-body{grid-template-columns:1fr} }
    .modal-photo{ border-radius:18px; overflow:hidden; border:1px solid var(--line); box-shadow:var(--shadow-soft); background:#ddd; aspect-ratio:4/5; }
    .modal-photo img{ width:100%; height:100%; object-fit:cover; display:block; }
    .modal-text p{margin:10px 0}
    .modal-sub{ margin:6px 0 0; color:var(--muted); font-weight:600; }

    .modal-actions{ padding:0 18px 18px; display:flex; justify-content:flex-end; align-items:center; gap:12px; flex-wrap:wrap; }
    .home-btn{
      display:inline-flex; align-items:center; justify-content:center; padding:12px 16px; border-radius:14px;
      border:1px solid rgba(0,0,0,.10); background:var(--gunn-red); color:#fff; font-weight:700; cursor:pointer;
      box-shadow:0 10px 22px rgba(0,0,0,.12); white-space:nowrap;
    }
    .home-btn:hover{background:var(--gunn-red-dark)}

    footer{
      background:linear-gradient(135deg,var(--gunn-red),var(--gunn-red-dark));
      color:#fff; margin-top:40px;
    }
    footer .container{
      padding:26px 20px; display:flex; justify-content:space-between; align-items:center; gap:16px; flex-wrap:wrap; font-weight:700;
    }
    footer .muted{opacity:.92;font-weight:700}
  </style>
</head>

<body>
  <nav class="nav" aria-label="Primary">
    <div class="container nav-inner">
      <a class="brand" href="#top">
        <img class="logo" src="./logo.png" alt="A2A-MML logo" />
        <span>A2A-MML · CVPR 2026</span>
      </a>
      <div class="menu">
        <a href="#call-for-papers">Call for Papers</a>
        <a href="#speakers">Speakers</a>
        <a href="#schedule">Tentative Schedule</a>
        <a href="#organizers">Organizers</a>
      </div>
    </div>
  </nav>

  <header id="top" class="hero">
    <div class="container wrap">
      <span class="badge">CVPR 2026 Workshop</span>
      <h1>WORKSHOP ON ANY-TO-ANY MULTIMODAL LEARNING</h1>
      <div class="cta">
        <a class="cta-btn" href="#call-for-papers">Call for Papers</a>
      </div>
    </div>
  </header>

  <main class="content">
    <section id="call-for-papers">
      <div class="container">
        <h2 class="section-title">Call for Papers</h2>

        <!-- ✅ 重要日期板块（替换你原来的那段文字+列表） -->
        <h3>Important Dates</h3>
        <p class="dates-note">Important Dates for Review Process. We will follow the suggested dates by CVPR.</p>

        <div class="dates-grid" aria-label="Important Dates">
          <div class="date-card">
            <p class="date">Mar 20, 2026</p>
            <p class="label">Workshop Paper Submission Deadline</p>
          </div>

          <div class="date-card">
            <p class="date">Apr 5, 2026</p>
            <p class="label">Workshop Paper Notification Date</p>
          </div>

          <div class="date-card">
            <p class="date">Apr 12, 2026</p>
            <p class="label">Program, Camera-ready, Videos Uploaded</p>
          </div>
        </div>

        <h3>Paper Submission and Acceptance</h3>
        <div class="box">
          <p>We welcome technical, position, or perspective papers related to the topics outlined in Section 1.2. All submissions must be written in English, follow the official CVPR proceedings format, and adhere to the double-blind review policy.</p>
          <ul class="bullets">
            <li><strong>Tiny or Short Papers (2–4 pages)</strong> - We invite concise papers that present implementations and evaluations of unpublished but insightful ideas, moderate yet self-contained theoretical analyses, follow-up experiments, re-analyses of prior work, or new perspectives on existing research.</li>
            <li><strong>Regular Papers (up to 8 pages, including figures and tables)</strong> - We encourage submissions introducing original methods, novel research visions, applications, or discussions of open challenges in multimodal learning.</li>
          </ul>
          <p>We plan to accept approximately 25 papers in total. All accepted papers will be presented as posters during the workshop, and 6–8 of them will be selected for short oral presentations. A Best Paper Award will be presented based on reviewer scores and the workshop committee’s evaluation.</p>
          <p>Poster sessions will be conducted onsite with dedicated time for interactive discussions. For remote attendees, we will offer a virtual poster gallery and live Q&amp;A channels to ensure inclusive engagement.</p>
        </div>

        <h3>Topics and Themes</h3>
        <div class="box">
          <p>We welcome all relevant submissions in the area of multimodal learning, with emphasis on any-to-any multimodal intelligence, such as:</p>
          <ul class="bullets">
            <li>Multimodal Representation Learning</li>
            <li>Multimodal Transformation</li>
            <li>Multimodal Synergistic Collaboration</li>
            <li>Benchmarking and Evaluation for Any-to-Any Multimodal Learning</li>
          </ul>

          <p>Other topics include, but are not limited to:</p>
          <ul class="bullets">
            <li>Unified multimodal foundation and agentic models.</li>
            <li>Representation learning for embodied and interactive systems.</li>
            <li>Integration of underexplored modalities and cognitive perspectives on multimodal perception and reasoning.</li>
          </ul>
        </div>

        <h3>About</h3>
        <div class="box">
          <p>The recent surge of multimodal large models has brought strong progress in connecting language, vision, audio, and beyond. Yet most existing systems remain constrained to fixed modality pairs, lacking flexibility to generalize or reason across arbitrary combinations. The Any-to-Any Multimodal Learning workshop aims to explore systems that can understand, align, transform, and generate across any set of modalities. We organize the discussion around three pillars: representation learning, transformation, and collaboration.</p>
        </div>
      </div>
    </section>

    <section id="speakers">
      <div class="container">
        <h2 class="section-title">Speakers</h2>
        <div style="height:16px"></div>
        <div id="speakersGrid" class="grid" aria-label="Invited Speakers"></div>
      </div>
    </section>

    <section id="schedule">
      <div class="container">
        <h2 class="section-title">Tentative Schedule</h2>
        <div style="height:16px"></div>

        <table class="schedule" aria-label="Workshop Schedule">
          <thead>
            <tr>
              <th style="width:180px">Time</th>
              <th>Schedule</th>
              <th style="width:160px">Speaker</th>
            </tr>
          </thead>
          <tbody>
            <tr class="section-row"><td colspan="3">Morning Schedule</td></tr>
            <tr><td>08:50 – 09:00</td><td>Introduction and opening remarks</td><td>-</td></tr>
            <tr><td>09:00 – 09:30</td><td>Keynote Talk 1</td><td>TBD</td></tr>
            <tr><td>09:30 – 10:00</td><td>Keynote Talk 2</td><td>TBD</td></tr>
            <tr><td>10:00 – 10:40</td><td>Oral Presentations</td><td>-</td></tr>
            <tr><td>10:40 – 11:00</td><td>Coffee Break</td><td>-</td></tr>
            <tr><td>11:00 – 11:30</td><td>Keynote Talk 3</td><td>TBD</td></tr>
            <tr><td>11:30 – 12:00</td><td>Keynote Talk 4</td><td>TBD</td></tr>
            <tr><td>12:00 – 12:30</td><td>Poster Session 1 (Interactive) + Virtual Gallery</td><td>-</td></tr>
            <tr><td>12:30 – 13:30</td><td>Lunch Break</td><td>-</td></tr>

            <tr class="section-row"><td colspan="3">Afternoon Schedule</td></tr>
            <tr><td>13:30 – 14:00</td><td>Keynote Talk 5</td><td>TBD</td></tr>
            <tr><td>14:00 – 14:30</td><td>Keynote Talk 6</td><td>TBD</td></tr>
            <tr><td>14:30 – 15:20</td><td>Poster Session 2 (Interactive) + Virtual Gallery</td><td>-</td></tr>
            <tr><td>15:20 – 15:50</td><td>Coffee Break</td><td>-</td></tr>
            <tr><td>15:50 – 16:20</td><td>Keynote Talk 7</td><td>TBD</td></tr>
            <tr><td>16:20 – 17:20</td><td>Panel Discussion</td><td>TBD</td></tr>
            <tr><td>17:20 – 17:30</td><td>Closing Remarks + Best Paper Award</td><td>TBD</td></tr>
          </tbody>
        </table>
      </div>
    </section>

    <section id="organizers">
      <div class="container">
        <h2 class="section-title">Organizers</h2>
        <div style="height:16px"></div>
        <div id="organizersGrid" class="grid" aria-label="Organizers"></div>
      </div>
    </section>
  </main>

  <footer>
    <div class="container">
      <div class="muted">CVPR 2026 Workshop on Any-to-Any Multimodal Learning</div>
      <div class="muted">A2A-MML</div>
    </div>
  </footer>

  <div id="modal" class="modal" role="dialog" aria-modal="true" aria-label="Profile">
    <div class="modal-panel">
      <div class="modal-head">
        <div>
          <h3 id="modalTitle" class="modal-title"></h3>
          <div id="modalSub" class="modal-sub"></div>
        </div>
        <button id="modalClose" class="modal-close" type="button" aria-label="Close">✕</button>
      </div>
      <div class="modal-body">
        <div class="modal-photo"><img id="modalImg" alt="" /></div>
        <div id="modalBio" class="modal-text"></div>
      </div>
      <div class="modal-actions">
        <a id="modalHome" class="home-btn" href="#" target="_blank" rel="noreferrer"></a>
      </div>
    </div>
  </div>

  <script>
    const ORGANIZERS = [
      { name:"Shengqiong Wu", affil:"National University of Singapore", img:"./Shengqiong.jpg", homepage:"https://sqwu.top/", bio:"Shengqiong Wu (she/her) is a senior Ph.D. student at the National University of Singapore, under the supervision of Prof. Tat-Seng Chua. Her research interests include multimodal learning and large vision-language foundation models. She has been recognized with Google Ph.D. Fellowship, Baidu Scholarship, and Bytedance Scholarship. She has co-organized workshops and grand challenges at ACM MM and WACV."},
      { name:"Wei Dai", affil:"MIT Media Lab", img:"./Wei_Dai.png", homepage:"https://dd.works/", bio:"Wei Dai (he/him) is a Ph.D. student at MIT Media Lab under Prof. Paul Liang. His research focuses on multimodal LLMs and healthcare. His work appears at NeurIPS, ICML, and MICCAI."},
      { name:"Han Lin", affil:"UNC Chapel Hill", img:"./Han_Lin.png", homepage:"https://hl-hanlin.github.io/", bio:"Han Lin (he/him) is a Ph.D. student at UNC Chapel Hill, supervised by Prof. Mohit Bansal. Research interests include image and video generation, multimodal learning, and LLMs."},
      { name:"Yichen Li", affil:"MIT", img:"./Yichen.jpeg", homepage:"https://scholar.google.com/citations?user=Lp7FBZ4AAAAJ&hl=en", bio:"Yichen Li (he/him) is a Ph.D. student at MIT with Prof. Antonio Torralba. Interests: multimodal learning and modality-agnostic foundations."},
      { name:"Chenyu Monica Wang", affil:"MIT", img:"./Chenyu.jpeg", homepage:"https://www.linkedin.com/in/chenyu-wang-3a6a9a193/?locale=zh_CN", bio:"Chenyu Monica Wang (she/her) is a Ph.D. student at MIT advised by Prof. Tommi Jaakkola. Interests: deep generative models, reinforcement learning, multi-modal learning, and AI for science."},
      { name:"Sharut Gupta", affil:"MIT", img:"./Sharut.jpg", homepage:"https://www.mit.edu/~sharut/", bio:"Sharut Gupta (she/her) is a Ph.D. student at MIT with Prof. Phillip Isola and Prof. Stefanie Jegelka. Interests: multi-modal representation learning, robustness, and OOD generalization."},
      { name:"Roman Bachmann", affil:"EPFL", img:"./Roman.jpg", homepage:"https://roman-bachmann.github.io/", bio:"Roman Bachmann (he/him) is a Ph.D. student at EPFL with Prof. Amir Zamir. Focus: scalable objectives for any-to-any models and flexible tokenizers."},
      { name:"Elisa Ricci", affil:"University of Trento", img:"./Elisa.png", homepage:"https://mhug.disi.unitn.it/people/elisa-ricci/", bio:"Elisa Ricci (she/her) is a Professor at the University of Trento. Research across computer vision, deep learning, and robotics perception."}
    ];

    const SPEAKERS = [
      { name:"Paul Liang", affil:"MIT", img:"./Paul_Liang.jpeg", homepage:"https://www.mit.edu/~ppliang/", bio:"Paul Pu Liang is an Assistant Professor at the MIT Media Lab and MIT EECS, and leads the Multisensory Intelligence research group, where he investigates how machines can learn from and reason across diverse sensory modalities to engage with the real world. He
received his PhD in Machine Learning from Carnegie Mellon University. His research focuses on
multimodal representation learning, modality transformation via unified architectures, and interactive
systems that bridge human and machine experience. He is dedicated to mentoring the next generation
of researchers and promoting broader participation in AI."},
      { name:"Manling Li", affil:"Northwestern University", img:"./Manling.jpeg", homepage:"https://limanling.github.io/", bio:"Manling Li is an Assistant Professor of Computer
Science at Northwestern University, where she leads the Machine Learning and Language (MLL) Lab. Her research explores how machines can reason, plan, and act across modalities—bridging
language, vision, audio, robotics, and embodied interaction—to build trustworthy, compositional, and
long-horizon intelligent systems. She holds a PhD from the University of Illinois Urbana-Champaign
and was a post-doctoral researcher at Stanford University. Prof. Manling has received many awards,
including the MIT Technology Review 35 Under 35, Microsoft Research PhD Fellowship, and the
ACL 2024 Outstanding Paper Award."},
      { name:"Mohit Bansal", affil:"University of North Carolina, Chapel Hill", img:"./Mohit.png", homepage:"https://www.cs.unc.edu/~mbansal/", bio:"Mohit Bansal is the John R. &
Louise S. Parker Distinguished Professor and the Director of the MURGe-Lab (UNC-NLP Group) in
the Computer Science department at UNC Chapel Hill. His research expertise is in natural language
processing and multimodal machine learning, with a particular focus on multimodal generative
models, grounded and embodied semantics, and interpretable, efficient, and generalizable deep
learning. He is an AAAI Fellow and recipient of the Presidential Early Career Award for Scientists
and Engineers (PECASE), IIT Kanpur Young Alumnus Award, and outstanding paper awards at ACL,
CVPR, and TMLR. His service includes EMNLP and CoNLL Program Co-Chair, ACM Doctoral
Dissertation Award Committee, and Associate/Action Editor for TACL, CL, and IEEE/ACM."},
      { name:"Zhedong Zheng", affil:"University of Macau", img:"./Zhedong.webp", homepage:"https://zdzheng.xyz/", bio:"Zhedong Zheng is an Assistant Professor with
the University of Macau. He received the Ph.D. degree from the University of Technology Sydney
in 2021 and the B.S. degree from Fudan University in 2016. He was a postdoctoral research fellow
at the School of Computing, National University of Singapore. He received the IEEE Circuits and
Systems Society Outstanding Young Author Award of 2021. His research interests include AIGC,
Data-centric AI, and Spatial Intelligence. He actively serves the academic community, acting as a
Senior PC for IJCAI and AAAI, an Area Chair for ACM MM’24, ACM MM’25 and ICASSP’25,
and the Publication Chair for ACM MM’25 and AVSS’25."},
      { name:"Yossi Gandelsman", affil:"Reqe", img:"./Yossi.png", homepage:"https://yossigandelsman.github.io/", bio:"Yossi Gandelsman is a research scientist at Reve. Starting Fall
2026, he will join the Toyota Technological Institute at Chicago (TTIC) as an assistant professor.
His research interests focus on interpretability and model mechanisms across computer vision and
language. He received his PhD from the University of California, Berkeley, worked on the Perception
Team at Google DeepMind, and holds an M.Sc. from the Weizmann Institute of Science."},
      { name:"Georgia Gkioxari", affil:"Caltech", img:"./Georgia.jpeg", homepage:"https://gkioxari.github.io/", bio:"Georgia Gkioxari is an Assistant Professor in the Computing
& Mathematical Sciences department at Caltech. Previously, she worked as a research scientist
at Meta FAIR. She obtained her PhD under Jitendra Malik at UC Berkeley, and completed her
undergraduate studies in Greece at the National Technical University of Athens. Her research focuses
on advanced visual perception, including 2D & 3D spatial representation and reasoning, transforming
images into structured multi-modal outputs. She has received numerous honors, including the PAMI
Young Researcher Award, Google Faculty Award, and Okawa Research Award."},
      { name:"Saining Xie", affil:"NYU", img:"./Saining.png", homepage:"https://sainingxie.com/", bio:"Saining Xie is an Assistant Professor of Computer Science at NYU’s
Courant Institute and a member of the NYU Center for Data Science. He earned his Ph.D. in
Computer Science & Engineering from UC San Diego and previously worked as a research scientist
at Facebook AI Research. His research develops robust, scalable visual-intelligence systems which
bridge visual perception, representation learning, and commonsense reasoning."}
    ];

    function escapeHtml(s){
      return String(s)
        .replaceAll("&","&amp;")
        .replaceAll("<","&lt;")
        .replaceAll(">","&gt;")
        .replaceAll('"',"&quot;")
        .replaceAll("'","&#039;");
    }

    function cardHTML(p){
      return `
        <div class="card" role="button" tabindex="0" data-name="${escapeHtml(p.name)}">
          <div class="img">
            <img src="${escapeHtml(p.img)}" alt="${escapeHtml(p.name)}" />
            <div class="chev" aria-hidden="true"><span>→</span></div>
          </div>
          <div class="meta">
            <p class="name">${escapeHtml(p.name)}</p>
            <p class="affil">${escapeHtml(p.affil || "")}</p>
          </div>
        </div>
      `;
    }

    function renderGrid(elId, arr, roleLabel){
      const el = document.getElementById(elId);
      el.innerHTML = arr.map(cardHTML).join("");
      el.querySelectorAll(".card").forEach(card=>{
        const name = card.getAttribute("data-name");
        const p = arr.find(x=>x.name===name);
        const open = ()=>openModal(p, roleLabel);
        card.addEventListener("click", open);
        card.addEventListener("keydown", e=>{
          if(e.key==="Enter" || e.key===" "){ e.preventDefault(); open(); }
        });
      });
    }

    const modal = document.getElementById("modal");
    const modalClose = document.getElementById("modalClose");
    const modalTitle = document.getElementById("modalTitle");
    const modalSub = document.getElementById("modalSub");
    const modalImg = document.getElementById("modalImg");
    const modalBio = document.getElementById("modalBio");
    const modalHome = document.getElementById("modalHome");

    function openModal(p, roleLabel){
      modalTitle.textContent = p.name || "";
      modalSub.textContent = p.affil || "";
      modalImg.src = p.img || "";
      modalImg.alt = p.name || "";
      modalBio.innerHTML = `<p>${escapeHtml(p.bio || "")}</p>`;
      if(p.homepage){
        modalHome.href = p.homepage;
        modalHome.textContent = `The ${roleLabel}'s Homepage`;
        modalHome.style.display = "inline-flex";
      }else{
        modalHome.href = "#";
        modalHome.textContent = `The ${roleLabel}'s Homepage`;
        modalHome.style.display = "none";
      }
      modal.classList.add("open");
      document.body.style.overflow = "hidden";
      modalClose.focus();
    }

    function closeModal(){
      modal.classList.remove("open");
      document.body.style.overflow = "";
    }

    modalClose.addEventListener("click", closeModal);
    modal.addEventListener("click", e=>{ if(e.target === modal) closeModal(); });
    document.addEventListener("keydown", e=>{ if(e.key === "Escape" && modal.classList.contains("open")) closeModal(); });

    renderGrid("speakersGrid", SPEAKERS, "Speaker");
    renderGrid("organizersGrid", ORGANIZERS, "Organizer");
  </script>
</body>
</html>
